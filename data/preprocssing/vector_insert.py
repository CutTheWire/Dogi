import json
import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple
import chromadb
from chromadb.config import Settings
from tqdm import tqdm
import time
import sys

# NumPy í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
import warnings
warnings.filterwarnings("ignore", message=".*NumPy.*")

# NumPy ë²„ì „ ê°•ì œ ì„¤ì •
os.environ['NUMPY_VERSION'] = '1.26.0'  

try:
    # NumPy 1.x ë²„ì „ ê°•ì œ ë¡œë“œ
    import numpy as np
    if hasattr(np, '__version__') and np.__version__.startswith('2.'):
        print("Warning: NumPy 2.x detected. Some features may not work properly.")
except ImportError:
    print("NumPy not available, continuing without it...")

# sentence-transformers importë¥¼ try-catchë¡œ ë³´í˜¸
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except Exception as e:
    print(f"Warning: sentence-transformers import failed: {e}")
    print("Falling back to alternative embedding method...")
    SENTENCE_TRANSFORMERS_AVAILABLE = False

import re

# í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
os.environ['ANONYMIZED_TELEMETRY'] = 'False'

# ë¡œê¹… ì„¤ì • - HTTP ë¡œê·¸ ìˆ¨ê¸°ê¸°
logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# httpx ë¡œê·¸ ë ˆë²¨ì„ WARNINGìœ¼ë¡œ ì„¤ì •í•˜ì—¬ INFO ë¡œê·¸ ìˆ¨ê¸°ê¸°
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("chromadb").setLevel(logging.WARNING)

def format_file_size(size_bytes: int) -> str:
    """íŒŒì¼ í¬ê¸°ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if size_bytes == 0:
        return "0B"
    size_names = ["B", "KB", "MB", "GB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    return f"{size_bytes:.2f}{size_names[i]}"

def print_progress_inline(current: int, total: int, prefix: str = "", suffix: str = "", length: int = 50):
    """ì¸ë¼ì¸ ì§„í–‰ë¥  í‘œì‹œ (ê°™ì€ ì¤„ì—ì„œ ì—…ë°ì´íŠ¸)"""
    percent = (current / total) * 100
    filled_length = int(length * current // total)
    bar = 'â–ˆ' * filled_length + '-' * (length - filled_length)
    
    # \rì„ ì‚¬ìš©í•˜ì—¬ ê°™ì€ ì¤„ì—ì„œ ì—…ë°ì´íŠ¸
    print(f'\r{prefix} |{bar}| {current}/{total} ({percent:.1f}%) {suffix}', end='', flush=True)
    
    # ì™„ë£Œ ì‹œ ì¤„ë°”ê¿ˆ
    if current == total:
        print()

def scan_directory_files(data_dir: str) -> Tuple[List[Path], Dict[str, Any]]:
    """ë””ë ‰í† ë¦¬ ì „ì²´ ìŠ¤ìº”í•˜ì—¬ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘"""
    data_path = Path(data_dir)
    all_files = []
    stats = {
        'total_files': 0,
        'total_size': 0,
        'corpus_files': 0,
        'corpus_size': 0,
        'qa_files': 0,
        'qa_size': 0,
        'departments': set(),
        'error_files': []
    }
    
    print("\nğŸ” ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì¤‘...")
    
    # ì›ì²œë°ì´í„°(ë§ë­‰ì¹˜) ìŠ¤ìº”
    corpus_dir = data_path / "1.ì›ì²œë°ì´í„°" / "ë§ë­‰ì¹˜ ë°ì´í„°"
    if corpus_dir.exists():
        print(f"ğŸ“ ì›ì²œë°ì´í„° ìŠ¤ìº”: {corpus_dir}")
        dept_dirs = list(corpus_dir.iterdir())
        
        for idx, dept_dir in enumerate(dept_dirs):
            if dept_dir.is_dir():
                # ì¸ë¼ì¸ ì§„í–‰ë¥  í‘œì‹œ
                print_progress_inline(idx + 1, len(dept_dirs), f"ğŸ“‚ {dept_dir.name}", "")
                
                stats['departments'].add(dept_dir.name)
                for json_file in dept_dir.glob("*.json"):
                    try:
                        file_size = json_file.stat().st_size
                        all_files.append(json_file)
                        stats['total_files'] += 1
                        stats['total_size'] += file_size
                        stats['corpus_files'] += 1
                        stats['corpus_size'] += file_size
                    except Exception as e:
                        stats['error_files'].append(str(json_file))
                        logger.warning(f"Error accessing file {json_file}: {e}")
    
    # ë¼ë²¨ë§ë°ì´í„°(ì§ˆì˜ì‘ë‹µ) ìŠ¤ìº”
    qa_dir = data_path / "2.ë¼ë²¨ë§ë°ì´í„°" / "ì§ˆì˜ì‘ë‹µ ë°ì´í„°"
    if qa_dir.exists():
        print(f"\nğŸ“ ë¼ë²¨ë§ë°ì´í„° ìŠ¤ìº”: {qa_dir}")
        dept_dirs = list(qa_dir.iterdir())
        
        for idx, dept_dir in enumerate(dept_dirs):
            if dept_dir.is_dir():
                # ì¸ë¼ì¸ ì§„í–‰ë¥  í‘œì‹œ
                print_progress_inline(idx + 1, len(dept_dirs), f"ğŸ“‚ {dept_dir.name}", "")
                
                stats['departments'].add(dept_dir.name)
                for json_file in dept_dir.glob("*.json"):
                    try:
                        file_size = json_file.stat().st_size
                        all_files.append(json_file)
                        stats['total_files'] += 1
                        stats['total_size'] += file_size
                        stats['qa_files'] += 1
                        stats['qa_size'] += file_size
                    except Exception as e:
                        stats['error_files'].append(str(json_file))
                        logger.warning(f"Error accessing file {json_file}: {e}")
    
    return all_files, stats

def print_scan_summary(stats: Dict[str, Any]):
    """ìŠ¤ìº” ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ“Š ë””ë ‰í† ë¦¬ ìŠ¤ìº” ê²°ê³¼")
    print("="*60)
    print(f"ğŸ“‚ ì´ íŒŒì¼ ìˆ˜: {stats['total_files']:,}ê°œ")
    print(f"ğŸ’¾ ì´ íŒŒì¼ í¬ê¸°: {format_file_size(stats['total_size'])}")
    print(f"ğŸ“‹ ì›ì²œë°ì´í„°(ë§ë­‰ì¹˜): {stats['corpus_files']:,}ê°œ ({format_file_size(stats['corpus_size'])})")
    print(f"â“ ë¼ë²¨ë§ë°ì´í„°(Q&A): {stats['qa_files']:,}ê°œ ({format_file_size(stats['qa_size'])})")
    print(f"ğŸ¥ ì§„ë£Œê³¼ëª©: {len(stats['departments'])}ê°œ - {', '.join(sorted(stats['departments']))}")
    
    if stats['error_files']:
        print(f"âš ï¸  ì ‘ê·¼ ì‹¤íŒ¨ íŒŒì¼: {len(stats['error_files'])}ê°œ")
    
    print("="*60)

class VetDataVectorizer:
    def __init__(self, 
                chroma_host: str = "localhost", 
                chroma_port: int = 8000,
                model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        ):
        """
        ìˆ˜ì˜í•™ ë°ì´í„° ë²¡í„°í™” í´ë˜ìŠ¤
        
        Args:
            chroma_host: ChromaDB í˜¸ìŠ¤íŠ¸
            chroma_port: ChromaDB í¬íŠ¸
            model_name: í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸
        """
        # HTTP ë¡œê·¸ ìˆ¨ê¸°ê¸°
        logging.getLogger("httpx").setLevel(logging.ERROR)
        logging.getLogger("chromadb").setLevel(logging.ERROR)
        
        self.client = chromadb.HttpClient(
            host=chroma_host,
            port=chroma_port,
            settings=Settings(
                allow_reset=True,
                anonymized_telemetry=False  # í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
            )
        )
        
        # í†µê³„ ì¶”ì 
        self.stats = {
            'processed_files': 0,
            'total_documents': 0,
            'corpus_documents': 0,
            'qa_documents': 0,
            'failed_files': 0,
            'processing_time': 0
        }
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = None
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                print("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.embedding_model = SentenceTransformer(model_name)
                print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
            except Exception as e:
                print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
                # ëŒ€ì•ˆ ëª¨ë¸ë“¤ ì‹œë„
                alternative_models = [
                    "sentence-transformers/all-MiniLM-L6-v2",
                    "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
                ]
                
                for alt_model in alternative_models:
                    try:
                        print(f"ğŸ”„ ëŒ€ì•ˆ ëª¨ë¸ ì‹œë„: {alt_model}")
                        self.embedding_model = SentenceTransformer(alt_model)
                        print(f"âœ… ëŒ€ì•ˆ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {alt_model}")
                        break
                    except Exception as alt_e:
                        print(f"âš ï¸ ëŒ€ì•ˆ ëª¨ë¸ ì‹¤íŒ¨ {alt_model}: {alt_e}")
                        continue
        
        if self.embedding_model is None:
            print("âš ï¸ ì„ë² ë”© ëª¨ë¸ ì—†ìŒ. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì‚¬ìš©.")
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        self.collection = self.client.get_or_create_collection(
            name="vet_medical_data",
            metadata={"description": "ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ë°ì´í„°"}
        )
        
    def simple_text_embedding(self, texts: List[str]) -> List[List[float]]:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© (fallback method)"""
        embeddings = []
        for text in texts:
            # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© (ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            text_hash = hash(text)
            # 384ì°¨ì› ë²¡í„° ìƒì„± (sentence-transformersì™€ í˜¸í™˜)
            embedding = [float((text_hash + i) % 1000) / 1000.0 for i in range(384)]
            embeddings.append(embedding)
        return embeddings
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # ì—°ì†ëœ ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', '\n', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•„ìš”ì— ë”°ë¼ ì¡°ì •)
        text = text.strip()
        
        return text
    
    def chunk_text(self, text: str, max_length: int = 500) -> List[str]:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• """
        if len(text) <= max_length:
            return [text]
        
        chunks = []
        sentences = text.split('.')
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) <= max_length:
                current_chunk += sentence + "."
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "."
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks
    
    def process_corpus_data(self, file_path: str) -> List[Dict[str, Any]]:
        """ì›ì²œë°ì´í„°(ë§ë­‰ì¹˜) ì²˜ë¦¬"""
        try:
            # UTF-8 BOM ì²˜ë¦¬ ê°œì„ 
            try:
                with open(file_path, 'r', encoding='utf-8-sig') as f:
                    data = json.load(f)
            except UnicodeDecodeError:
                # ëŒ€ì•ˆ ì¸ì½”ë”© ì‹œë„
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            
            documents = []
            
            # ì œëª©, ì €ì, ì¶œíŒì‚¬ ì •ë³´
            title = data.get('title', '')
            author = data.get('author', '')
            publisher = data.get('publisher', '')
            department = data.get('department', '')
            disease_content = data.get('disease', '')
            
            # ì§ˆë³‘ ë‚´ìš©ì„ ì²­í¬ë¡œ ë¶„í• 
            cleaned_content = self.clean_text(disease_content)
            chunks = self.chunk_text(cleaned_content, max_length=500)
            
            for i, chunk in enumerate(chunks):
                if len(chunk.strip()) < 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                    continue
                    
                doc = {
                    'id': f"{Path(file_path).stem}_chunk_{i}",
                    'content': chunk,
                    'metadata': {
                        'source_type': 'corpus',
                        'title': title,
                        'author': author,
                        'publisher': publisher,
                        'department': department,
                        'file_path': file_path,
                        'chunk_index': i
                    }
                }
                documents.append(doc)
                
            return documents
            
        except Exception as e:
            # ì—ëŸ¬ë„ ì¡°ìš©íˆ ì²˜ë¦¬
            self.stats['failed_files'] += 1
            return []
    
    def process_qa_data(self, file_path: str) -> List[Dict[str, Any]]:
        """ë¼ë²¨ë§ë°ì´í„°(ì§ˆì˜ì‘ë‹µ) ì²˜ë¦¬"""
        try:
            try:
                with open(file_path, 'r', encoding='utf-8-sig') as f:
                    data = json.load(f)
            except UnicodeDecodeError:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            
            documents = []
            
            # ë©”íƒ€ë°ì´í„°
            meta = data.get('meta', {})
            qa = data.get('qa', {})
            
            # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë³„ë„ ë¬¸ì„œë¡œ ì €ì¥
            instruction = qa.get('instruction', '')
            user_input = qa.get('input', '')
            output = qa.get('output', '')
            
            # ì§ˆë¬¸ ë¬¸ì„œ
            if user_input:
                doc_question = {
                    'id': f"{Path(file_path).stem}_question",
                    'content': f"{instruction}\n\nì§ˆë¬¸: {user_input}",
                    'metadata': {
                        'source_type': 'qa_question',
                        'life_cycle': meta.get('lifeCycle', ''),
                        'department': meta.get('department', ''),
                        'disease': meta.get('disease', ''),
                        'file_path': file_path,
                        'content_type': 'question'
                    }
                }
                documents.append(doc_question)
            
            # ë‹µë³€ ë¬¸ì„œ
            if output:
                doc_answer = {
                    'id': f"{Path(file_path).stem}_answer",
                    'content': f"ë‹µë³€: {output}",
                    'metadata': {
                        'source_type': 'qa_answer',
                        'life_cycle': meta.get('lifeCycle', ''),
                        'department': meta.get('department', ''),
                        'disease': meta.get('disease', ''),
                        'file_path': file_path,
                        'content_type': 'answer',
                        'related_question': user_input
                    }
                }
                documents.append(doc_answer)
            
            return documents
            
        except Exception as e:
            # ì—ëŸ¬ë„ ì¡°ìš©íˆ ì²˜ë¦¬
            self.stats['failed_files'] += 1
            return []
    
    def insert_documents(self, documents: List[Dict[str, Any]]):
        """ë¬¸ì„œë“¤ì„ ChromaDBì— ì‚½ì…"""
        if not documents:
            return
        
        try:
            # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
            texts = [doc['content'] for doc in documents]
            
            if self.embedding_model is not None:
                try:
                    embeddings = self.embedding_model.encode(texts, show_progress_bar=False)
                    # NumPy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (NumPy í˜¸í™˜ì„± ë¬¸ì œ ë°©ì§€)
                    if hasattr(embeddings, 'tolist'):
                        embeddings = embeddings.tolist()
                    else:
                        embeddings = list(embeddings)
                except Exception as e:
                    embeddings = self.simple_text_embedding(texts)
            else:
                embeddings = self.simple_text_embedding(texts)
            
            # ChromaDBì— ì‚½ì…
            self.collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=[doc['metadata'] for doc in documents],
                ids=[doc['id'] for doc in documents]
            )
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats['total_documents'] += len(documents)
            if documents[0]['metadata']['source_type'].startswith('corpus'):
                self.stats['corpus_documents'] += len(documents)
            else:
                self.stats['qa_documents'] += len(documents)
            
        except Exception as e:
            # ì—ëŸ¬ë„ ì¡°ìš©íˆ ì²˜ë¦¬
            pass
    
    def process_files_with_progress(self, file_list: List[Path]):
        """ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ íŒŒì¼ë“¤ ì²˜ë¦¬ (ê¹”ë”í•œ ì¸ë¼ì¸ ë²„ì „)"""
        start_time = time.time()
        total_files = len(file_list)
        
        print(f"\nğŸš€ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ (ì´ {total_files:,}ê°œ íŒŒì¼)")
        print("ğŸ’¡ HTTP ë¡œê·¸ê°€ ìˆ¨ê²¨ì¡ŒìŠµë‹ˆë‹¤. ê¹”ë”í•œ ì§„í–‰ë¥  í‘œì‹œë¥¼ ì¦ê¸°ì„¸ìš”!")
        
        for idx, file_path in enumerate(file_list):
            # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ
            filename = file_path.name
            if len(filename) > 25:
                filename = filename[:22] + "..."
            
            # ì¸ë¼ì¸ ì§„í–‰ë¥  í‘œì‹œ
            elapsed = time.time() - start_time
            rate = (idx + 1) / elapsed if elapsed > 0 else 0
            eta = (total_files - idx - 1) / rate if rate > 0 else 0
            
            progress_info = f"ğŸ“„ {filename} | ğŸ“Š {self.stats['total_documents']:,}docs | âš¡ {rate:.1f}f/s | â° {eta:.0f}s"
            print_progress_inline(idx + 1, total_files, "ğŸ”„", progress_info)
            
            # íŒŒì¼ ìœ í˜•ì— ë”°ë¼ ì²˜ë¦¬
            if "ë§ë­‰ì¹˜ ë°ì´í„°" in str(file_path):
                documents = self.process_corpus_data(str(file_path))
            elif "ì§ˆì˜ì‘ë‹µ ë°ì´í„°" in str(file_path):
                documents = self.process_qa_data(str(file_path))
            else:
                continue
            
            if documents:
                self.insert_documents(documents)
            
            self.stats['processed_files'] += 1
        
        self.stats['processing_time'] = time.time() - start_time
        print(f"\nâœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
    
    def get_collection_info(self):
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥"""
        count = self.collection.count()
        
        print("\n" + "="*60)
        print("ğŸ“Š ë²¡í„°í™” ì™„ë£Œ ê²°ê³¼")
        print("="*60)
        print(f"âœ… ì´ ì²˜ë¦¬ëœ ë¬¸ì„œ: {self.stats['total_documents']:,}ê°œ")
        print(f"ğŸ“‹ ì›ì²œë°ì´í„° ë¬¸ì„œ: {self.stats['corpus_documents']:,}ê°œ")
        print(f"â“ ì§ˆì˜ì‘ë‹µ ë¬¸ì„œ: {self.stats['qa_documents']:,}ê°œ")
        print(f"ğŸ“ ì²˜ë¦¬ëœ íŒŒì¼: {self.stats['processed_files']:,}ê°œ")
        print(f"âŒ ì‹¤íŒ¨í•œ íŒŒì¼: {self.stats['failed_files']:,}ê°œ")
        print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {self.stats['processing_time']:.2f}ì´ˆ")
        print(f"ğŸš€ í‰ê·  ì†ë„: {self.stats['processed_files']/self.stats['processing_time']:.1f} íŒŒì¼/ì´ˆ")
        print(f"ğŸ—„ï¸  ChromaDB ì €ì¥ëœ ë¬¸ì„œ: {count:,}ê°œ")
        print("="*60)
        
        # ìƒ˜í”Œ ë¬¸ì„œ ì¡°íšŒ
        if count > 0:
            print("\nğŸ“„ ìƒ˜í”Œ ë¬¸ì„œ:")
            sample = self.collection.peek(limit=3)
            for i, (doc, metadata) in enumerate(zip(sample['documents'], sample['metadatas'])):
                print(f"\në¬¸ì„œ {i+1}: {doc[:100]}...")
                print(f"ë©”íƒ€ë°ì´í„°: {metadata}")
    
    def search_similar(self, query: str, n_results: int = 5, department: str = None):
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        if self.embedding_model is not None:
            try:
                query_embedding = self.embedding_model.encode([query], show_progress_bar=False)
                if hasattr(query_embedding, 'tolist'):
                    query_embedding = query_embedding.tolist()
                else:
                    query_embedding = list(query_embedding)
            except Exception as e:
                query_embedding = self.simple_text_embedding([query])
        else:
            query_embedding = self.simple_text_embedding([query])
        
        where_clause = {}
        if department:
            where_clause["department"] = department
        
        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=n_results,
            where=where_clause if where_clause else None
        )
        
        return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ• ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ë°ì´í„° ë²¡í„°í™” ì‹œìŠ¤í…œ")
    print("="*60)
    
    current_file = Path(__file__).resolve()
    data_dir = current_file.parents[1] / "raw"
    
    print(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    
    # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
    if not data_dir.exists():
        print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_dir}")
        return
    
    # 1ë‹¨ê³„: ì „ì²´ íŒŒì¼ ìŠ¤ìº”
    file_list, scan_stats = scan_directory_files(str(data_dir))
    print_scan_summary(scan_stats)
    
    if not file_list:
        print("âŒ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2ë‹¨ê³„: VectorDB ì´ˆê¸°í™”
    print("\nğŸš€ ë²¡í„°í™” ì‹œì‘...")
    vectorizer = VetDataVectorizer(
        chroma_host="localhost",  # Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ì‹œ "chromadb"ë¡œ ë³€ê²½
        chroma_port=8000
    )
    
    # 3ë‹¨ê³„: íŒŒì¼ ì²˜ë¦¬ (ê¹”ë”í•œ ì¸ë¼ì¸ ì§„í–‰ë¥  í‘œì‹œ)
    vectorizer.process_files_with_progress(file_list)
    
    # 4ë‹¨ê³„: ê²°ê³¼ í™•ì¸
    vectorizer.get_collection_info()
    
    # 5ë‹¨ê³„: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("="*60)
    test_query = "ê°•ì•„ì§€ íŒŒë³´ì¥ì—¼ ì¹˜ë£Œ"
    results = vectorizer.search_similar(test_query, n_results=3)
    
    print(f"\nğŸ” ê²€ìƒ‰ì–´: '{test_query}'")
    print("-" * 60)
    
    for i, (doc, metadata, distance) in enumerate(zip(
        results['documents'][0], 
        results['metadatas'][0], 
        results['distances'][0]
    )):
        print(f"\nğŸ“‹ ê²°ê³¼ {i+1} (ìœ ì‚¬ë„: {1-distance:.4f}):")
        print(f"ğŸ“„ ë‚´ìš©: {doc[:200]}...")
        print(f"ğŸ¥ ì§„ë£Œê³¼: {metadata.get('department', 'N/A')}")
        print(f"ğŸ“ ìœ í˜•: {metadata.get('source_type', 'N/A')}")

if __name__ == "__main__":
    main()