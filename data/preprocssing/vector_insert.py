import json
import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple
import chromadb
from chromadb.config import Settings
from tqdm import tqdm
import time
import sys

# NumPy í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
import warnings
warnings.filterwarnings("ignore", message=".*NumPy.*")

# NumPy ë²„ì „ ê°•ì œ ì„¤ì •
os.environ['NUMPY_VERSION'] = '1.26.0'  

try:
    # NumPy 1.x ë²„ì „ ê°•ì œ ë¡œë“œ
    import numpy as np
    if hasattr(np, '__version__') and np.__version__.startswith('2.'):
        print("Warning: NumPy 2.x detected. Some features may not work properly.")
except ImportError:
    print("NumPy not available, continuing without it...")

# sentence-transformers importë¥¼ try-catchë¡œ ë³´í˜¸
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except Exception as e:
    print(f"Warning: sentence-transformers import failed: {e}")
    print("Falling back to alternative embedding method...")
    SENTENCE_TRANSFORMERS_AVAILABLE = False

import re

# í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
os.environ['ANONYMIZED_TELEMETRY'] = 'False'

# ë¡œê¹… ì„¤ì • - HTTP ë¡œê·¸ ìˆ¨ê¸°ê¸°
logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# httpx ë¡œê·¸ ë ˆë²¨ì„ WARNINGìœ¼ë¡œ ì„¤ì •í•˜ì—¬ INFO ë¡œê·¸ ìˆ¨ê¸°ê¸°
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("chromadb").setLevel(logging.WARNING)

def print_progress_inline(
        current: int,
        total: int,
        prefix: str = "",
        suffix: str = "",
    ) -> None:
    """
    ê°„ë‹¨í•œ ì§„í–‰ë¥  í‘œì‹œ (ì‚½ì…ëœ íŒŒì¼ ìˆ˜/ì „ì²´ íŒŒì¼ ìˆ˜)
    
    Args:
        current (int): í˜„ì¬ ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜
        total (int): ì „ì²´ íŒŒì¼ ìˆ˜
        prefix (str): ì§„í–‰ë¥  ì•ì— í‘œì‹œí•  ë¬¸ìì—´
        suffix (str): ì§„í–‰ë¥  ë’¤ì— í‘œì‹œí•  ë¬¸ìì—´
        length (int): ì§„í–‰ë¥  í‘œì‹œ ê¸¸ì´ (ê¸°ë³¸ê°’: 50)
    """
    print(f'\r{prefix} {current}/{total} íŒŒì¼ ì²˜ë¦¬ ì¤‘... {suffix}', end='', flush=True)
    if current == total:
        print(f'\r{prefix} {current}/{total} íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ! {suffix}', end='', flush=True)

def scan_directory_files(data_dir: str) -> Tuple[List[Path], Dict[str, Any]]:
    """
    ë””ë ‰í† ë¦¬ ì „ì²´ ìŠ¤ìº”í•˜ì—¬ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘

    Args:
        data_dir (str): ìŠ¤ìº”í•  ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        Tuple[List[Path], Dict[str, Any]]: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ì™€ í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    data_path = Path(data_dir)
    all_files = []
    stats = {
        'total_files': 0,
        'total_size': 0,
        'corpus_files': 0,
        'corpus_size': 0,
        'qa_files': 0,
        'qa_size': 0,
        'departments': set(),
        'error_files': []
    }

    print("\n ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì¤‘...")
    
    # í™•ì¸í•  ê²½ë¡œ ëª©ë¡
    check_dirs = {
        "ì›ì²œë°ì´í„°(ë§ë­‰ì¹˜)": data_path / "1.ì›ì²œë°ì´í„°" / "ë§ë­‰ì¹˜ ë°ì´í„°",
        "ë¼ë²¨ë§ë°ì´í„°(QA)": data_path / "2.ë¼ë²¨ë§ë°ì´í„°" / "ì§ˆì˜ì‘ë‹µ ë°ì´í„°"
    }

    # ì—†ëŠ” ë””ë ‰í† ë¦¬ë§Œ í•„í„°ë§
    missing = list(filter(lambda item: not item[1].exists(), check_dirs.items()))

    if missing:
        for name, path in missing:
            print(f" ê²½ê³ : {name} ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.: {path}")
        print("ì›ì²œë°ì´í„°(ë§ë­‰ì¹˜)ì™€ ë¼ë²¨ë§ë°ì´í„°(QA)ê°€ ë‘˜ ë‹¤ ìˆì–´ì•¼ ì§„í–‰ ë©ë‹ˆë‹¤. ")
        return all_files, stats 
    
    # ì›ì²œë°ì´í„°(ë§ë­‰ì¹˜) ì²˜ë¦¬
    corpus_dept_dirs = list(filter(
        lambda p: p.is_dir(), check_dirs["ì›ì²œë°ì´í„°(ë§ë­‰ì¹˜)"].iterdir()
    ))
    
    for idx, dept_dir in enumerate(corpus_dept_dirs):
        # ì§„í–‰ë¥  í‘œì‹œ
        print_progress_inline(
            current = idx + 1,
            total = len(corpus_dept_dirs),
            prefix = f" {dept_dir.name}",
            suffix = ""
        )
        
        stats['departments'].add(dept_dir.name)
        for json_file in dept_dir.glob("*.json"):
            try:
                file_size = json_file.stat().st_size
                all_files.append(json_file)
                stats['total_files'] += 1
                stats['total_size'] += file_size
                stats['corpus_files'] += 1
                stats['corpus_size'] += file_size
            except Exception as e:
                stats['error_files'].append(str(json_file))
                logger.warning(f"Error accessing file {json_file}: {e}")
    
    # ë¼ë²¨ë§ë°ì´í„°(QA) ì²˜ë¦¬
    qa_dept_dirs = list(filter(
        lambda p: p.is_dir(), check_dirs["ë¼ë²¨ë§ë°ì´í„°(QA)"].iterdir()
    ))
    
    for idx, dept_dir in enumerate(qa_dept_dirs):
        # ì§„í–‰ë¥  í‘œì‹œ
        print_progress_inline(
            current = idx + 1,
            total = len(qa_dept_dirs),
            prefix = f" {dept_dir.name}",
            suffix = ""
        )
        
        stats['departments'].add(dept_dir.name)
        for json_file in dept_dir.glob("*.json"):
            try:
                file_size = json_file.stat().st_size
                all_files.append(json_file)
                stats['total_files'] += 1
                stats['total_size'] += file_size
                stats['qa_files'] += 1
                stats['qa_size'] += file_size
            except Exception as e:
                stats['error_files'].append(str(json_file))
                logger.warning(f"Error accessing file {json_file}: {e}")


    return all_files, stats

def print_scan_summary(stats: Dict[str, Any]):
    """
    ìŠ¤ìº” ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    
    Args:
        stats (Dict[str, Any]): ìŠ¤ìº” ê²°ê³¼ í†µê³„ ì •ë³´
    """
    print("\n" + "="*60)
    print(" ë””ë ‰í† ë¦¬ ìŠ¤ìº” ê²°ê³¼")
    print("="*60)
    print(f" ì´ íŒŒì¼ ìˆ˜: {stats['total_files']:,}ê°œ")
    print(f" ì´ íŒŒì¼ í¬ê¸°: {stats['total_size']}")
    print(f" ì›ì²œë°ì´í„°(ë§ë­‰ì¹˜): {stats['corpus_files']:,}ê°œ ({stats['corpus_size']})")
    print(f" ë¼ë²¨ë§ë°ì´í„°(Q&A): {stats['qa_files']:,}ê°œ ({stats['qa_size']})")
    print(f" ì§„ë£Œê³¼ëª©: {len(stats['departments'])}ê°œ - {', '.join(sorted(stats['departments']))}")
    
    if stats['error_files']:
        print(f"  ì ‘ê·¼ ì‹¤íŒ¨ íŒŒì¼: {len(stats['error_files'])}ê°œ")
    
    print("="*60)

class VetDataVectorizer:
    def __init__(self, 
            chroma_host: str = "localhost", 
            chroma_port: int = 7999,
            model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        ):
        """
        ìˆ˜ì˜í•™ ë°ì´í„° ë²¡í„°í™” í´ë˜ìŠ¤
        
        Args:
            chroma_host: ChromaDB í˜¸ìŠ¤íŠ¸
            chroma_port: ChromaDB í¬íŠ¸
            model_name: í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸
        """
        # HTTP ë¡œê·¸ ìˆ¨ê¸°ê¸°
        logging.getLogger("httpx").setLevel(logging.ERROR)
        logging.getLogger("chromadb").setLevel(logging.ERROR)
        
        self.client = chromadb.HttpClient(
            host=chroma_host,
            port=chroma_port,
            settings=Settings(
                allow_reset=True,
                anonymized_telemetry=False  # í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
            )
        )
        
        # í†µê³„ ì¶”ì 
        self.stats = {
            'processed_files': 0,
            'total_documents': 0,
            'corpus_documents': 0,
            'qa_documents': 0,
            'failed_files': 0,
            'failed_file_list': [],  # ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡ ì¶”ê°€
            'processing_time': 0
        }
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = None
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                print(" ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
                self.embedding_model = SentenceTransformer(model_name)
                print(f" ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
                
            except Exception as e:
                print(f" ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_name}: {e}")
                # ëŒ€ì•ˆ ëª¨ë¸ë“¤ ì‹œë„
                alternative_models = [
                    "sentence-transformers/all-MiniLM-L6-v2",
                    "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
                ]
                
                for alt_model in alternative_models:
                    try:
                        print(f" ëŒ€ì•ˆ ëª¨ë¸ ì‹œë„: {alt_model}")
                        self.embedding_model = SentenceTransformer(alt_model)
                        print(f" ëŒ€ì•ˆ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {alt_model}")
                        break
                    except Exception as alt_e:
                        print(f" ëŒ€ì•ˆ ëª¨ë¸ ì‹¤íŒ¨ {alt_model}: {alt_e}")
                        continue
        
        if self.embedding_model is None:
            print(" ì„ë² ë”© ëª¨ë¸ ì—†ìŒ. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì‚¬ìš©.")
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        self.collection = self.client.get_or_create_collection(
            name="vet_medical_data",
            metadata={"description": "ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ë°ì´í„°"}
        )
        
    def simple_text_embedding(self, texts: List[str]) -> List[List[float]]:
        """
        ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© (fallback method)
        
        Args:
            texts (List[str]): í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            List[List[float]]: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        embeddings = []
        for text in texts:
            # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”©
            text_hash = hash(text)
            # 384ì°¨ì› ë²¡í„° ìƒì„± (sentence-transformersì™€ í˜¸í™˜)
            embedding = [float((text_hash + i) % 1000) / 1000.0 for i in range(384)]
            embeddings.append(embedding)
        
        return embeddings
    
    def clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        
        Args:
            text (str): ì›ë³¸ í…ìŠ¤íŠ¸
        
        Returns:
            str: ì •ì œëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return ""
        
        # ì—°ì†ëœ ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', '\n', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•„ìš”ì— ë”°ë¼ ì¡°ì •)
        text = text.strip()
        
        return text
    
    def chunk_text(self, text: str, max_length: int = 500) -> List[str]:
        """
        ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
        
        Args:
            text (str): ì›ë³¸ í…ìŠ¤íŠ¸
            max_length (int): ìµœëŒ€ ì²­í¬ ê¸¸ì´
        
        Returns:
            List[str]: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if len(text) <= max_length:
            return [text]
        
        chunks = []
        sentences = text.split('.')
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) <= max_length:
                current_chunk += sentence + "."
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "."
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks
    
    def safe_load_json(self, file_path: str) -> Dict[str, Any]:
        """
        ì•ˆì „í•œ JSON íŒŒì¼ ë¡œë“œ (ì—¬ëŸ¬ ì¸ì½”ë”© ë° ì—ëŸ¬ ì²˜ë¦¬)
        
        Args:
            file_path (str): JSON íŒŒì¼ ê²½ë¡œ
        
        Returns:
            Dict[str, Any]: íŒŒì‹±ëœ JSON ë°ì´í„° (ì‹¤íŒ¨ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬)
        """
        encodings = ['utf-8-sig', 'utf-8', 'cp949', 'euc-kr']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    data = json.load(f)
                    return data
            except UnicodeDecodeError:
                continue
            except json.JSONDecodeError as e:
                # JSON íŒŒì‹± ì—ëŸ¬ë¥¼ ë” ìì„¸íˆ ë¡œê¹…
                logger.warning(f"JSON parsing error in {file_path}: {e}")
                # ì†ìƒëœ JSON íŒŒì¼ ë³µêµ¬ ì‹œë„
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                        # ê°„ë‹¨í•œ JSON ë³µêµ¬ ì‹œë„ (ì˜ˆ: ë§ˆì§€ë§‰ ì½¤ë§ˆ ì œê±°)
                        content = content.rstrip()
                        if content.endswith(','):
                            content = content[:-1]
                        data = json.loads(content)
                        return data
                except:
                    continue
            except Exception as e:
                logger.warning(f"Unexpected error loading {file_path}: {e}")
                continue
        
        # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
        self.stats['failed_files'] += 1
        self.stats['failed_file_list'].append(file_path)
        return {}
    
    def process_corpus_data(self, file_path: str) -> List[Dict[str, Any]]:
        """
        ì›ì²œë°ì´í„°(ë§ë­‰ì¹˜) ì²˜ë¦¬
        
        Args:
            file_path (str): ì²˜ë¦¬í•  JSON íŒŒì¼ ê²½ë¡œ
        
        Returns:
            List[Dict[str, Any]]: ì²˜ë¦¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        data = self.safe_load_json(file_path)
        if not data:
            return []
        
        try:
            documents = []
            
            # ì œëª©, ì €ì, ì¶œíŒì‚¬ ì •ë³´
            title = data.get('title', '')
            author = data.get('author', '')
            publisher = data.get('publisher', '')
            department = data.get('department', '')
            disease_content = data.get('disease', '')
            
            # disease_contentê°€ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
            if not isinstance(disease_content, str):
                disease_content = str(disease_content) if disease_content else ""
            
            # ì§ˆë³‘ ë‚´ìš©ì„ ì²­í¬ë¡œ ë¶„í• 
            cleaned_content = self.clean_text(disease_content)
            if not cleaned_content:
                return []
                
            chunks = self.chunk_text(cleaned_content, max_length=500)
            
            for i, chunk in enumerate(chunks):
                if len(chunk.strip()) < 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                    continue
                    
                doc = {
                    'id': f"{Path(file_path).stem}_chunk_{i}",
                    'content': chunk,
                    'metadata': {
                        'source_type': 'corpus',
                        'title': title,
                        'author': author,
                        'publisher': publisher,
                        'department': department,
                        'file_path': file_path,
                        'chunk_index': i
                    }
                }
                documents.append(doc)
                
            return documents
            
        except Exception as e:
            logger.warning(f"Error processing corpus file {file_path}: {e}")
            self.stats['failed_files'] += 1
            self.stats['failed_file_list'].append(file_path)
            return []
    
    def process_qa_data(self, file_path: str) -> List[Dict[str, Any]]:
        """
        ë¼ë²¨ë§ë°ì´í„°(ì§ˆì˜ì‘ë‹µ) ì²˜ë¦¬
        
        Args:
            file_path (str): ì²˜ë¦¬í•  JSON íŒŒì¼ ê²½ë¡œ
        
        Returns:
            List[Dict[str, Any]]: ì²˜ë¦¬ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        data = self.safe_load_json(file_path)
        if not data:
            return []
            
        try:
            documents = []
            
            # ë©”íƒ€ë°ì´í„°
            meta = data.get('meta', {})
            qa = data.get('qa', {})
            
            # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë³„ë„ ë¬¸ì„œë¡œ ì €ì¥
            instruction = qa.get('instruction', '')
            user_input = qa.get('input', '')
            output = qa.get('output', '')
            
            # ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
            instruction = str(instruction) if instruction else ""
            user_input = str(user_input) if user_input else ""
            output = str(output) if output else ""
            
            # ì§ˆë¬¸ ë¬¸ì„œ
            if user_input and len(user_input.strip()) > 10:
                doc_question = {
                    'id': f"{Path(file_path).stem}_question",
                    'content': f"{instruction}\n\nì§ˆë¬¸: {user_input}".strip(),
                    'metadata': {
                        'source_type': 'qa_question',
                        'life_cycle': meta.get('lifeCycle', ''),
                        'department': meta.get('department', ''),
                        'disease': meta.get('disease', ''),
                        'file_path': file_path,
                        'content_type': 'question'
                    }
                }
                documents.append(doc_question)
            
            # ë‹µë³€ ë¬¸ì„œ
            if output and len(output.strip()) > 10:
                doc_answer = {
                    'id': f"{Path(file_path).stem}_answer",
                    'content': f"ë‹µë³€: {output}",
                    'metadata': {
                        'source_type': 'qa_answer',
                        'life_cycle': meta.get('lifeCycle', ''),
                        'department': meta.get('department', ''),
                        'disease': meta.get('disease', ''),
                        'file_path': file_path,
                        'content_type': 'answer',
                        'related_question': user_input
                    }
                }
                documents.append(doc_answer)
            
            return documents
            
        except Exception as e:
            logger.warning(f"Error processing QA file {file_path}: {e}")
            self.stats['failed_files'] += 1
            self.stats['failed_file_list'].append(file_path)
            return []
    
    def insert_documents(self, documents: List[Dict[str, Any]]):
        """
        ë¬¸ì„œë“¤ì„ ChromaDBì— ì‚½ì…
        
        Args:
            documents (List[Dict[str, Any]]): ì‚½ì…í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not documents:
            return
        
        try:
            # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
            texts = [doc['content'] for doc in documents]
            
            if self.embedding_model:
                try:
                    embeddings = self.embedding_model.encode(texts, show_progress_bar=False)
                    embeddings = embeddings.tolist() if hasattr(embeddings, 'tolist') else list(embeddings)
                except Exception:
                    embeddings = self.simple_text_embedding(texts)
            else:
                embeddings = self.simple_text_embedding(texts)

            # ChromaDBì— ì‚½ì…
            self.collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=[doc['metadata'] for doc in documents],
                ids=[doc['id'] for doc in documents]
            )
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats['total_documents'] += len(documents)
            if documents[0]['metadata']['source_type'].startswith('corpus'):
                self.stats['corpus_documents'] += len(documents)
            else:
                self.stats['qa_documents'] += len(documents)
            
        except Exception as e:
            logger.warning(f"Error inserting documents: {e}")
    
    def process_files_with_progress(self, file_list: List[Path]):
        """
        ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ íŒŒì¼ë“¤ ì²˜ë¦¬ (ê¹”ë”í•œ ì¸ë¼ì¸ ë²„ì „)
        
        Args:
            file_list (List[Path]): ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        start_time = time.time()
        total_files = len(file_list)
        
        print(f"\n íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ (ì´ {total_files:,}ê°œ íŒŒì¼)")
        print(" HTTP ë¡œê·¸ê°€ ìˆ¨ê²¨ì¡ŒìŠµë‹ˆë‹¤. ê¹”ë”í•œ ì§„í–‰ë¥  í‘œì‹œë¥¼ ì¦ê¸°ì„¸ìš”!")
        
        for idx, file_path in enumerate(file_list):
            # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ
            filename = file_path.name
            if len(filename) > 25:
                filename = filename[:22] + "..."
            
            # ì¸ë¼ì¸ ì§„í–‰ë¥  í‘œì‹œ
            elapsed = time.time() - start_time
            rate = (idx + 1) / elapsed if elapsed > 0 else 0
            eta = (total_files - idx - 1) / rate if rate > 0 else 0
            
            progress_info = f" {filename} | {self.stats['total_documents']:,}docs | {rate:.1f}f/s | {eta:.0f}s"
            
            # ì§„í–‰ë¥  í‘œì‹œ
            print_progress_inline(
                current = idx + 1,
                total = total_files,
                prefix = "ğŸ”„",
                suffix = progress_info
            )
            
            # íŒŒì¼ ìœ í˜•ì— ë”°ë¼ ì²˜ë¦¬
            try:
                if "ë§ë­‰ì¹˜ ë°ì´í„°" in str(file_path):
                    documents = self.process_corpus_data(str(file_path))
                elif "ì§ˆì˜ì‘ë‹µ ë°ì´í„°" in str(file_path):
                    documents = self.process_qa_data(str(file_path))
                else:
                    continue
                
                if documents:
                    self.insert_documents(documents)
                
                self.stats['processed_files'] += 1
                
            except Exception as e:
                logger.warning(f"Unexpected error processing {file_path}: {e}")
                self.stats['failed_files'] += 1
                self.stats['failed_file_list'].append(str(file_path))
                continue
        
        self.stats['processing_time'] = time.time() - start_time
        print(f"\n íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
    
    def get_collection_info(self):
        """
        ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        """
        count = self.collection.count()
        
        print("\n" + "="*60)
        print(" ë²¡í„°í™” ì™„ë£Œ ê²°ê³¼")
        print("="*60)
        print(f" ì´ ì²˜ë¦¬ëœ ë¬¸ì„œ: {self.stats['total_documents']:,}ê°œ")
        print(f" ì›ì²œë°ì´í„° ë¬¸ì„œ: {self.stats['corpus_documents']:,}ê°œ")
        print(f" ì§ˆì˜ì‘ë‹µ ë¬¸ì„œ: {self.stats['qa_documents']:,}ê°œ")
        print(f" ì„±ê³µ ì²˜ë¦¬ íŒŒì¼: {self.stats['processed_files']:,}ê°œ")
        print(f" ì‹¤íŒ¨í•œ íŒŒì¼: {self.stats['failed_files']:,}ê°œ")
        print(f" ì²˜ë¦¬ ì‹œê°„: {self.stats['processing_time']:.2f}ì´ˆ")
        print(f" í‰ê·  ì†ë„: {self.stats['processed_files']/self.stats['processing_time']:.1f} íŒŒì¼/ì´ˆ")
        print(f" ChromaDB ì €ì¥ëœ ë¬¸ì„œ: {count:,}ê°œ")
        
        # ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡ í‘œì‹œ (ì²˜ìŒ 5ê°œë§Œ)
        if self.stats['failed_file_list']:
            print(f"\n ì‹¤íŒ¨í•œ íŒŒì¼ ì˜ˆì‹œ (ì²˜ìŒ 5ê°œ):")
            for failed_file in self.stats['failed_file_list'][:5]:
                print(f"  - {Path(failed_file).name}")
            if len(self.stats['failed_file_list']) > 5:
                print(f"  ... ë° {len(self.stats['failed_file_list']) - 5}ê°œ ë”")
        
        print("="*60)
        
        # ìƒ˜í”Œ ë¬¸ì„œ ì¡°íšŒ
        if count > 0:
            print("\n ìƒ˜í”Œ ë¬¸ì„œ:")
            sample = self.collection.peek(limit=3)
            for i, (doc, metadata) in enumerate(zip(sample['documents'], sample['metadatas'])):
                print(f"\në¬¸ì„œ {i+1}: {doc[:100]}...")
                print(f"ë©”íƒ€ë°ì´í„°: {metadata}")
    
    def search_similar(self,
            query: str,
            n_results: int = 5,
            department: str = None
        ) -> Dict[str, Any]:
        """
        ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query (str): ê²€ìƒ‰í•  ì§ˆì˜ì–´
            n_results (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            department (str): íŠ¹ì • ì§„ë£Œê³¼ëª© í•„í„°ë§ (ì„ íƒì )
        
        Returns:
            Dict[str, Any]: ê²€ìƒ‰ ê²°ê³¼ (ë¬¸ì„œ, ë©”íƒ€ë°ì´í„°, ê±°ë¦¬)
        """
        if self.embedding_model is not None:
            try:
                query_embedding = self.embedding_model.encode([query], show_progress_bar=False)
                query_embedding = query_embedding.tolist() if hasattr(query_embedding, 'tolist') else list(query_embedding)
            except Exception as e:
                query_embedding = self.simple_text_embedding([query])
        else:
            query_embedding = self.simple_text_embedding([query])
        
        where_clause = {}
        if department:
            where_clause["department"] = department
        
        results = self.collection.query(
            query_embeddings = query_embedding,
            n_results = n_results,
            where = where_clause if where_clause else None
        )
        
        return results

if __name__ == "__main__":
    print(" ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ë°ì´í„° ë²¡í„°í™” ì‹œìŠ¤í…œ")
    print("="*60)
    
    current_file = Path(__file__).resolve()
    data_dir = current_file.parents[1] / "raw"
    
    print(f" ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    
    # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
    if not data_dir.exists():
        print(f" ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_dir}")
        sys.exit(1)
        
    # 1ë‹¨ê³„: ì „ì²´ íŒŒì¼ ìŠ¤ìº”
    file_list, scan_stats = scan_directory_files(str(data_dir))
    print_scan_summary(scan_stats)
    
    if not file_list:
        print(" ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    # 2ë‹¨ê³„: VectorDB ì´ˆê¸°í™”
    print("\n ë²¡í„°í™” ì‹œì‘...")
    vectorizer = VetDataVectorizer(
        chroma_host="localhost",  # Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ì‹œ "chromadb"ë¡œ ë³€ê²½
        chroma_port=7999
    )
    
    # 3ë‹¨ê³„: íŒŒì¼ ì²˜ë¦¬ (ê¹”ë”í•œ ì¸ë¼ì¸ ì§„í–‰ë¥  í‘œì‹œ)
    vectorizer.process_files_with_progress(file_list)
    
    # 4ë‹¨ê³„: ê²°ê³¼ í™•ì¸
    vectorizer.get_collection_info()
    
    # 5ë‹¨ê³„: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("="*60)
    test_query = "ê°•ì•„ì§€ íŒŒë³´ì¥ì—¼ ì¹˜ë£Œ"
    results = vectorizer.search_similar(test_query, n_results=3)
    
    print(f"\n ê²€ìƒ‰ì–´: '{test_query}'")
    print("-" * 60)
    
    for i, (doc, metadata, distance) in enumerate(zip(
        results['documents'][0], 
        results['metadatas'][0], 
        results['distances'][0]
    )):
        print(f"\n ê²°ê³¼ {i+1} (ìœ ì‚¬ë„: {1-distance:.4f}):")
        print(f" ë‚´ìš©: {doc[:200]}...")
        print(f" ì§„ë£Œê³¼: {metadata.get('department', 'N/A')}")
        print(f" ìœ í˜•: {metadata.get('source_type', 'N/A')}")
