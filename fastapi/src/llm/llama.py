'''
íŒŒì¼ì€ LlamaModel, BaseConfig.OfficePrompt í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê³  llama_cpp_cudaë¥¼ ì‚¬ìš©í•˜ì—¬,
Meta-Llama-3.1-8B-Claude.Q4_0.gguf ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
ChromaDBëŠ” LangChainìœ¼ë¡œ ì—°ê²°í•˜ê³ , ëª¨ë¸ì€ llama_cpp_cudaë¡œ ì§ì ‘ ì„œë¹™í•©ë‹ˆë‹¤.
'''
from typing import  Generator, List, Dict
from llama_cpp_cuda import Llama

import os
import sys
import json
import textwrap
import warnings
import time
from queue import Queue
from threading import Thread
from contextlib import contextmanager
from datetime import datetime

from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.schema.retriever import BaseRetriever
from langchain.callbacks.manager import CallbackManagerForRetrieverRun
from pydantic import Field

from service.vector_client import VectorSearchHandler
from domain import BaseConfig
from core import app_state as AppState

GREEN = "\033[32m"
RED = "\033[31m"
YELLOW = "\033[33m"
BLUE = "\033[34m"
RESET = "\033[0m"

class VectorRetriever(BaseRetriever):
    """
    VectorSearchHandlerë¥¼ LangChain Retrieverë¡œ ë˜í•‘ (ChromaDB ì—°ê²°ìš©)
    """
    vector_handler: VectorSearchHandler = Field(description="ChromaDB ë²¡í„° ê²€ìƒ‰ í•¸ë“¤ëŸ¬")
    
    class Config:
        arbitrary_types_allowed = True
    
    def __init__(self, vector_handler: VectorSearchHandler, **kwargs):
        super().__init__(vector_handler=vector_handler, **kwargs)
    
    def _get_relevant_documents(
        self, 
        query: str, 
        *, 
        run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """
        VectorSearchHandlerì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ LangChain Documentë¡œ ë³€í™˜
        """
        if not self.vector_handler:
            print("    âš ï¸ VectorSearchHandlerê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            print(f"    ğŸ” ChromaDB ë²¡í„° ê²€ìƒ‰ ì¤‘: '{query[:50]}...'")
            
            # ë§ë­‰ì¹˜ ë°ì´í„° ê²€ìƒ‰
            corpus_results = self.vector_handler.search_relevant_documents(
                query=query,
                n_results=3,
                source_type="corpus"
            )
            
            # Q&A ë°ì´í„° ê²€ìƒ‰
            qa_results = self.vector_handler.search_relevant_documents(
                query=query,
                n_results=2,
                source_type="qa_answer"
            )
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            all_results = corpus_results + qa_results
            
            print(f"    ğŸ“„ {len(all_results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨ (ë§ë­‰ì¹˜: {len(corpus_results)}, Q&A: {len(qa_results)})")
            
            # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            documents = []
            for result in all_results:
                doc = Document(
                    page_content=result.get('content', ''),
                    metadata={
                        **result.get('metadata', {}),
                        'similarity': result.get('similarity', 0),
                        'rank': result.get('rank', 0)
                    }
                )
                documents.append(doc)
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            documents.sort(key=lambda x: x.metadata.get('similarity', 0), reverse=True)
            
            return documents[:5]
            
        except Exception as e:
            print(f"    âŒ ChromaDB ë²¡í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

def build_rag_prompt_template() -> PromptTemplate:
    """
    RAGë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± (LangChain í…œí”Œë¦¿ í˜•ì‹)
    """
    template = textwrap.dedent("""
        <|begin_of_text|><|start_header_id|>system<|end_header_id|>
        ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ìƒë‹´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì•„ë˜ ChromaDBì—ì„œ ê²€ìƒ‰ëœ ì˜ë£Œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

        ê²€ìƒ‰ëœ ì˜ë£Œ ì •ë³´:
        {context}

        ëŒ€í™” ê¸°ë¡:
        {chat_history}

        ì§€ì‹œ ì‚¬í•­:
        - í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
        - ê²€ìƒ‰ëœ ì˜ë£Œ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”
        - ì˜ë£Œ ì •ë³´ëŠ” ì •í™•í•˜ê³  ì‹ ì¤‘í•˜ê²Œ ì œê³µí•˜ë©°, ì‘ê¸‰ìƒí™©ì´ë‚˜ ì‹¬ê°í•œ ì¦ìƒì˜ ê²½ìš° ì¦‰ì‹œ ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œìœ í•˜ì„¸ìš”
        - ê²€ìƒ‰ëœ ì •ë³´ê°€ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ì—†ë‹¤ë©´ ì¼ë°˜ì ì¸ ì˜ë£Œ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
        - ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ë„ë¡ í•˜ì„¸ìš”<|eot_id|>

        <|start_header_id|>user<|end_header_id|>
        {question}<|eot_id|>

        <|start_header_id|>assistant<|end_header_id|>
    """).strip()
        
    return PromptTemplate(
        template=template,
        input_variables=["context", "chat_history", "question"]
    )

class LlamaModel:
    """
    llama_cpp_cudaë¡œ ëª¨ë¸ ì„œë¹™ + LangChainìœ¼ë¡œ ChromaDB ì—°ê²°í•˜ëŠ” RAG ì‹œìŠ¤í…œ
    """
    def __init__(self) -> None:
        """
        LlamaModel í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì†Œë“œ
        """
        self.model_id = 'llama-3-Korean-Bllossom-8B'
        self.model_path = "/app/fastapi/models/llama-3-Korean-Bllossom-8B.gguf"
        self.file_path = "/app/prompt/config-Llama.json"
        self.loading_text = f"{BLUE}LOADING{RESET}:    {self.model_id} ë¡œë“œ ì¤‘..."
        
        print("\n"+ f"{BLUE}LOADING{RESET}:  " + "="*len(self.loading_text))
        print(f"{BLUE}LOADING{RESET}:    {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")

        # JSON íŒŒì¼ ì½ê¸°
        with open(self.file_path, 'r', encoding = 'utf-8') as file:
            self.data: BaseConfig.BaseConfig = json.load(file)

        # llama_cpp_cudaë¡œ ëª¨ë¸ ë¡œë“œ
        print(f"{BLUE}LOADING{RESET}:    llama_cpp_cudaë¡œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.model: Llama = self._load_model()
        print(f"{BLUE}LOADING{RESET}:    llama_cpp_cuda ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        
        # ChromaDB + LangChain RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_rag_components()
        
        print(f"{BLUE}LOADING{RESET}:  " + "="*len(self.loading_text) + "\n")
        
        self.response_queue: Queue = Queue()

    def _initialize_rag_components(self):
        """
        ChromaDB ì—°ê²° + LangChain RAG ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”
        """
        try:
            # VectorSearchHandler ì´ˆê¸°í™” (app_stateì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            self.vector_handler = AppState.get_vector_handler()
            
            if self.vector_handler and self.vector_handler.health_check():
                print(f"{BLUE}LOADING{RESET}:    ChromaDB ì—°ê²° í™•ì¸ ì™„ë£Œ!")
                
                # LangChain Retriever ì´ˆê¸°í™” (ChromaDB ì—°ê²°ìš©)
                self.retriever = VectorRetriever(vector_handler=self.vector_handler)
                
                # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
                self.prompt_template = build_rag_prompt_template()
                
                self.rag_available = True
                print(f"{BLUE}LOADING{RESET}:    LangChain RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ!")
                
            else:
                print(f"{YELLOW}WARNING{RESET}:  ChromaDB ì—°ê²° ì‹¤íŒ¨, RAG ê¸°ëŠ¥ ì œí•œë¨")
                self.retriever = None
                self.prompt_template = None
                self.rag_available = False
                
        except Exception as e:
            print(f"{RED}ERROR{RESET}:     RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.retriever = None
            self.prompt_template = None
            self.rag_available = False

    def _format_documents(self, docs: List[Document]) -> str:
        """
        LangChain Documentë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
        """
        if not docs:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_docs = []
        corpus_docs = []
        qa_docs = []
        
        # ë¬¸ì„œ íƒ€ì…ë³„ë¡œ ë¶„ë¥˜
        for doc in docs:
            source_type = doc.metadata.get('source_type', '')
            if source_type == 'corpus':
                corpus_docs.append(doc)
            elif source_type in ['qa_answer', 'qa_question']:
                qa_docs.append(doc)
            else:
                corpus_docs.append(doc)
        
        # ë§ë­‰ì¹˜ ë¬¸ì„œ í¬ë§·íŒ…
        if corpus_docs:
            formatted_docs.append("=== ê´€ë ¨ ì˜ë£Œ ì •ë³´ (ë§ë­‰ì¹˜ ë°ì´í„°) ===")
            for i, doc in enumerate(corpus_docs, 1):
                content = doc.page_content[:500]
                metadata = doc.metadata
                
                doc_info = f"[ë¬¸ì„œ {i}]"
                if metadata.get('department'):
                    doc_info += f" (ì§„ë£Œê³¼: {metadata['department']})"
                if metadata.get('similarity'):
                    doc_info += f" (ìœ ì‚¬ë„: {metadata['similarity']:.3f})"
                
                formatted_docs.append(f"{doc_info}\n{content}")
        
        # Q&A ë¬¸ì„œ í¬ë§·íŒ…
        if qa_docs:
            formatted_docs.append("\n=== ê´€ë ¨ ì§ˆì˜ì‘ë‹µ (Q&A ë°ì´í„°) ===")
            for i, doc in enumerate(qa_docs, 1):
                content = doc.page_content[:300]
                metadata = doc.metadata
                
                doc_info = f"[Q&A {i}]"
                if metadata.get('department'):
                    doc_info += f" (ì§„ë£Œê³¼: {metadata['department']})"
                if metadata.get('similarity'):
                    doc_info += f" (ìœ ì‚¬ë„: {metadata['similarity']:.3f})"
                
                formatted_docs.append(f"{doc_info}\n{content}")
        
        return "\n\n".join(formatted_docs)

    def _format_chat_history(self, chat_list: List[Dict]) -> str:
        """
        ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
        """
        if not chat_list:
            return "ì´ì „ ëŒ€í™” ì—†ìŒ"
        
        formatted_history = []
        for chat in chat_list[-3:]:
            user_msg = chat.get("content", chat.get("input_data", ""))
            ai_msg = chat.get("answer", chat.get("output_data", ""))
            
            if user_msg:
                formatted_history.append(f"ì‚¬ìš©ì: {user_msg}")
            if ai_msg:
                formatted_history.append(f"AI: {ai_msg}")
        
        return "\n".join(formatted_history)

    def _load_model(self) -> Llama:
        """
        llama_cpp_cudaë¥¼ ì‚¬ìš©í•´ GGUF í¬ë§·ì˜ Llama ëª¨ë¸ì„ ë¡œë“œ
        """
        print(f"{self.loading_text}")
        try:
            warnings.filterwarnings("ignore")
            
            @contextmanager
            def suppress_stdout():
                with open(os.devnull, "w") as devnull:
                    old_stdout = sys.stdout
                    sys.stdout = devnull
                    try:
                        yield
                    finally:
                        sys.stdout = old_stdout

            # GPU ì‚¬ìš©ëŸ‰ ê·¹ëŒ€í™”ë¥¼ ìœ„í•œ ì„¤ì •
            with suppress_stdout():
                model = Llama(
                    model_path = self.model_path,       # GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
                    n_gpu_layers = -1,                  # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUì— ë¡œë“œ
                    main_gpu = 0,                       # 0ë²ˆ GPU ì‚¬ìš© (ìˆ˜ì •)
                    rope_scaling_type = 2,              # RoPE ìŠ¤ì¼€ì¼ë§ ë°©ì‹ (2 = linear) 
                    rope_freq_scale = 2.0,              # RoPE ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ â†’ ê¸´ ë¬¸ë§¥ ì§€ì›   
                    n_ctx = 8191,                       # ìµœëŒ€ context length
                    n_batch = 2048,                     # ë°°ì¹˜ í¬ê¸° (VRAM ì œí•œ ê³ ë ¤í•œ ì¤‘ê°„ ê°’)
                    verbose = False,                    # ë””ë²„ê¹… ë¡œê·¸ ë¹„í™œì„±í™”  
                    offload_kqv = True,                 # K/Q/V ìºì‹œë¥¼ CPUë¡œ ì˜¤í”„ë¡œë“œí•˜ì—¬ VRAM ì ˆì•½
                    use_mmap = False,                   # ë©”ëª¨ë¦¬ ë§¤í•‘ ë¹„í™œì„±í™” 
                    use_mlock = True,                   # ë©”ëª¨ë¦¬ ì ê¸ˆìœ¼ë¡œ ë©”ëª¨ë¦¬ í˜ì´ì§€ ìŠ¤ì™‘ ë°©ì§€
                    n_threads = 12,                     # CPU ìŠ¤ë ˆë“œ ìˆ˜ (ì½”ì–´ 12ê°œ ê¸°ì¤€ ì ì ˆí•œ ê°’)
                    tensor_split = [1.0],               # ë‹¨ì¼ GPUì—ì„œ ëª¨ë“  í…ì„œ ë¡œë”©
                    split_mode = 1,                     # í…ì„œ ë¶„í•  ë°©ì‹ (1 = ê· ë“± ë¶„í• )
                    flash_attn = True,                  # FlashAttention ì‚¬ìš© (ì†ë„ í–¥ìƒ)
                    cont_batching = True,               # ì—°ì† ë°°ì¹­ í™œì„±í™” (ë©€í‹° ì‚¬ìš©ì ì²˜ë¦¬ì— íš¨ìœ¨ì )
                    numa = False,                       # NUMA ë¹„í™œì„±í™” (ë‹¨ì¼ GPU ì‹œìŠ¤í…œì—ì„œ ë¶ˆí•„ìš”)
                    f16_kv = True,                      # 16bit KV ìºì‹œ ì‚¬ìš©
                    logits_all = False,                 # ë§ˆì§€ë§‰ í† í°ë§Œ logits ê³„ì‚°
                    embedding = False,                  # ì„ë² ë”© ë¹„í™œì„±í™”
                )
            return model
        except Exception as e:
            print(f"âŒ llama_cpp_cuda ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise e

    def generate_response(self, input_text: str, chat_list: List[Dict]) -> str:
        """
        ChromaDB RAG + llama_cpp_cuda ëª¨ë¸ì„ í™œìš©í•œ ì‘ë‹µ ìƒì„±

        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            chat_list (List[Dict]): ëŒ€í™” ê¸°ë¡

        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        chunks = []
        for chunk in self.generate_response_stream(input_text, chat_list):
            chunks.append(chunk)
        return "".join(chunks)

    def generate_response_stream(self, input_text: str, chat_list: List[Dict]) -> Generator[str, None, None]:
        """
        ChromaDB RAG + llama_cpp_cudaë¥¼ í™œìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±

        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            chat_list (List[Dict]): ëŒ€í™” ê¸°ë¡

        Yields:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤
        """
        start_time = time.time()
        try:
            print(f"    ğŸš€ ChromaDB RAG + llama_cpp_cuda ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹œì‘...")
            
            # RAG ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if self.rag_available and self.retriever and self.prompt_template:
                # LangChain Retrieverë¡œ ChromaDBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
                docs = self.retriever.get_relevant_documents(input_text)
                context = self._format_documents(docs)
                
                # ëŒ€í™” ê¸°ë¡ í¬ë§·íŒ…
                chat_history = self._format_chat_history(chat_list)
                
                # LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = self.prompt_template.format(
                    context=context,
                    chat_history=chat_history,
                    question=input_text
                )
                
                print(f"    ğŸ” ChromaDB RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...")
                
                # llama_cpp_cudaë¡œ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
                config = BaseConfig.LlamaGenerationConfig(
                    prompt=prompt,
                    max_tokens=1024,
                    temperature=0.7,
                    top_p=0.9,
                    stop=["<|eot_id|>"]
                )
                
                # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
                for text_chunk in self.create_streaming_completion(config):
                    yield text_chunk
                    
                generation_time = time.time() - start_time
                print(f"    âœ… ChromaDB RAG + llama_cpp_cuda ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ (ì†Œìš” ì‹œê°„: {generation_time:.2f}ì´ˆ)")
                
            else:
                print(f"    âš ï¸ ChromaDB RAG ê¸°ëŠ¥ ì‚¬ìš© ë¶ˆê°€, llama_cpp_cuda ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜")
                # í´ë°± ìŠ¤íŠ¸ë¦¬ë° (ìˆœìˆ˜ llama_cpp_cuda)
                for text_chunk in self._generate_fallback_response_stream(input_text, chat_list):
                    yield text_chunk

        except Exception as e:
            generation_time = time.time() - start_time
            print(f"âŒ ChromaDB RAG ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} (ì†Œìš” ì‹œê°„: {generation_time:.2f}ì´ˆ)")
            # ì—ëŸ¬ ì‹œ í´ë°± ìŠ¤íŠ¸ë¦¬ë°
            for text_chunk in self._generate_fallback_response_stream(input_text, chat_list):
                yield text_chunk

    def _generate_fallback_response_stream(self, input_text: str, chat_list: List[Dict]) -> Generator[str, None, None]:
        """
        ChromaDB RAG ì‹¤íŒ¨ì‹œ ìˆœìˆ˜ llama_cpp_cudaë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        """
        try:
            print(f"    ğŸ”„ ìˆœìˆ˜ llama_cpp_cuda ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‘ë‹µ ìƒì„±...")
            
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
            current_time = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_prompt = textwrap.dedent(f"""
                ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ìƒë‹´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                í˜„ì¬ ì‹œê°„: {current_time}

                ì§€ì‹œ ì‚¬í•­:
                - í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
                - ì˜ë£Œ ì •ë³´ëŠ” ì •í™•í•˜ê³  ì‹ ì¤‘í•˜ê²Œ ì œê³µí•˜ë©°, ì‘ê¸‰ìƒí™©ì´ë‚˜ ì‹¬ê°í•œ ì¦ìƒì˜ ê²½ìš° ì¦‰ì‹œ ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œìœ í•˜ì„¸ìš”
                - ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ë„ë¡ í•˜ì„¸ìš”
            """).strip()

            # ëŒ€í™” ê¸°ë¡ í¬ë§·íŒ…
            chat_history = self._format_chat_history(chat_list)
            
            # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = textwrap.dedent(f"""
                <|begin_of_text|><|start_header_id|>system<|end_header_id|>
                {system_prompt}

                ëŒ€í™” ê¸°ë¡:
                {chat_history}<|eot_id|>

                <|start_header_id|>user<|end_header_id|>
                {input_text}<|eot_id|>

                <|start_header_id|>assistant<|end_header_id|>
            """).strip()
            
            # llama_cpp_cuda ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •
            config = BaseConfig.LlamaGenerationConfig(
                prompt=prompt,
                max_tokens=1024,
                temperature=0.7,
                top_p=0.9,
                stop=["<|eot_id|>"]
            )
            
            # llama_cpp_cuda ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
            for text_chunk in self.create_streaming_completion(config):
                yield text_chunk
        
        except Exception as e:
            print(f"âŒ ìˆœìˆ˜ llama_cpp_cuda ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _stream_completion(self, config: BaseConfig.LlamaGenerationConfig) -> None:
        """
        llama_cpp_cudaë¡œ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ì–´ ì‘ë‹µì„ íì— ë„£ëŠ” ë©”ì„œë“œ (ìŠ¤íŠ¸ë¦¬ë°ìš©)
        """
        try:
            # llama_cpp_cuda ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
            stream = self.model.create_completion(
                prompt = config.prompt,
                max_tokens = config.max_tokens,
                temperature = config.temperature,
                top_p = config.top_p,
                min_p = config.min_p,
                typical_p = config.typical_p,
                tfs_z = config.tfs_z,
                repeat_penalty = config.repeat_penalty,
                frequency_penalty = config.frequency_penalty,
                presence_penalty = config.presence_penalty,
                stop = config.stop or ["<|eot_id|>"],
                stream = True,
                seed = config.seed,
            )
            
            token_count = 0
            for output in stream:
                if 'choices' in output and len(output['choices']) > 0:
                    text = output['choices'][0].get('text', '')
                    if text:
                        self.response_queue.put(text)
                        token_count += 1
                        
            print(f"    llama_cpp_cuda ìƒì„±ëœ í† í° ìˆ˜: {token_count}")
            self.response_queue.put(None)  # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
            
        except Exception as e:
            print(f"llama_cpp_cuda ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.response_queue.put(None)

    def create_streaming_completion(self, config: BaseConfig.LlamaGenerationConfig) -> Generator[str, None, None]:
        """
        llama_cpp_cudaë¡œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±
        """
        # í ì´ˆê¸°í™”
        while not self.response_queue.empty():
            self.response_queue.get()
            
        # llama_cpp_cuda ìŠ¤íŠ¸ë¦¬ë° ìŠ¤ë ˆë“œ ì‹œì‘
        thread = Thread(
            target = self._stream_completion,
            args = (config,)
        )
        thread.start()

        # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        token_count = 0
        while True:
            text = self.response_queue.get()
            if text is None:
                break
            token_count += 1
            yield text
            
        # ìŠ¤ë ˆë“œê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        thread.join()
        print(f"    llama_cpp_cuda ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {token_count}ê°œ í† í° ìˆ˜ì‹ ")

