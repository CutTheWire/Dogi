'''
íŒŒì¼ì€ LlamaModel, BaseConfig.OfficePrompt í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê³  llama_cpp_cudaë¥¼ ì‚¬ìš©í•˜ì—¬,
Meta-Llama-3.1-8B-Claude.Q4_0.gguf ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
RAG(Retrieval-Augmented Generation) ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.
'''
from typing import Optional, Generator, List, Dict
from llama_cpp_cuda import Llama

import os
import sys
import json
import warnings
import time
from queue import Queue
from threading import Thread
from contextlib import contextmanager
from datetime import datetime

from domain import BaseConfig

GREEN = "\033[32m"
RED = "\033[31m"
YELLOW = "\033[33m"
BLUE = "\033[34m"
RESET = "\033[0m"

def build_llama3_prompt(
        character_info: BaseConfig.OfficePrompt, 
        vector_context: str = ""
    ) -> str:
    """
    RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ Llama3 GGUF í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        character_info (BaseConfig.OfficePrompt): ìºë¦­í„° ê¸°ë³¸ ì •ë³´ ë° ëŒ€í™” ë§¥ë½ í¬í•¨ ê°ì²´
        vector_context (str): ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì–»ì€ ê´€ë ¨ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸

    Returns:
        str: Llama3 GGUF í¬ë§·ìš© í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    # ë²¡í„° ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ì™€ ê¸°ì¡´ ì°¸ê³  ì •ë³´ ê²°í•©
    reference_data = vector_context if vector_context else character_info.reference_data
    
    system_prompt = (
        f"ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ {character_info.name}ì…ë‹ˆë‹¤.\n"
        f"ë‹¹ì‹ ì˜ ì—­í• : {character_info.context}\n\n"
        f"ì°¸ê³  ì •ë³´ (ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆì„ ê²½ìš°ì—ë§Œ í™œìš©í•˜ì„¸ìš”):\n"
        f"{reference_data}\n\n"
        f"ì§€ì‹œ ì‚¬í•­:\n"
        f"- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”\n"
        f"- ì¹œì ˆí•˜ê³  ìœ ìµí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”\n"
        f"- ìœ„ì˜ ì°¸ê³  ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ë˜, ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ì •ë³´ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”\n"
        f"- ì˜ë£Œ ì •ë³´ëŠ” ì •í™•í•˜ê³  ì‹ ì¤‘í•˜ê²Œ ì œê³µí•˜ë©°, ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œìœ í•˜ì„¸ìš”\n"
        f"- ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ë„ë¡ í•˜ì„¸ìš”\n"
    )

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‹œì‘
    prompt = (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        f"{system_prompt}<|eot_id|>"
    )

    # ëŒ€í™” ê¸°ë¡ ì¶”ê°€
    if character_info.chat_list:
        for chat in character_info.chat_list:
            user_input = chat.get("content", chat.get("input_data", ""))
            assistant_output = chat.get("answer", chat.get("output_data", ""))

            if user_input:
                prompt += (
                    "<|start_header_id|>user<|end_header_id|>\n"
                    f"{user_input}<|eot_id|>"
                )
            if assistant_output:
                prompt += (
                    "<|start_header_id|>assistant<|end_header_id|>\n"
                    f"{assistant_output}<|eot_id|>"
                )

    # ìµœì‹  ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    prompt += (
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{character_info.user_input}<|eot_id|>"
        "<|start_header_id|>assistant<|end_header_id|>\n"
    )

    return prompt

class LlamaModel:
    """
    GGUF í¬ë§· llama-3-Korean-Bllossom-8B ëª¨ë¸ í´ë˜ìŠ¤
    ì™¸ë¶€ì—ì„œ ì œê³µë°›ì€ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def __init__(self, enable_rag: bool = True) -> None:
        """
        LlamaModel í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì†Œë“œ
        
        Args:
            enable_rag: RAG ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€ (í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ì§€ë§Œ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        """
        self.model_id = 'llama-3-Korean-Bllossom-8B'
        self.model_path = "/app/fastapi/models/llama-3-Korean-Bllossom-8B.gguf"
        self.file_path = "/app/prompt/config-Llama.json"
        self.loading_text = f"{BLUE}LOADING{RESET}:    {self.model_id} ë¡œë“œ ì¤‘..."
        self.character_info: Optional[BaseConfig.OfficePrompt] = None
        self.config: Optional[BaseConfig.LlamaGenerationConfig] = None
        
        print("\n"+ f"{BLUE}LOADING{RESET}:  " + "="*len(self.loading_text))
        print(f"{BLUE}LOADING{RESET}:    {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")

        # JSON íŒŒì¼ ì½ê¸°
        with open(self.file_path, 'r', encoding = 'utf-8') as file:
            self.data: BaseConfig.BaseConfig = json.load(file)

        # ì§„í–‰ ìƒíƒœ í‘œì‹œ
        print(f"{BLUE}LOADING{RESET}:    {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.model: Llama = self._load_model()
        print(f"{BLUE}LOADING{RESET}:    ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print(f"{BLUE}LOADING{RESET}:  " + "="*len(self.loading_text) + "\n")
        
        self.response_queue: Queue = Queue()

    def _load_model(self) -> Llama:
        """
        GGUF í¬ë§·ì˜ Llama ëª¨ë¸ì„ ë¡œë“œí•˜ê³  GPU ê°€ì†ì„ ìµœëŒ€í™”í•©ë‹ˆë‹¤.
        """
        print(f"{self.loading_text}")
        try:
            warnings.filterwarnings("ignore")
            
            @contextmanager
            def suppress_stdout():
                with open(os.devnull, "w") as devnull:
                    old_stdout = sys.stdout
                    sys.stdout = devnull
                    try:
                        yield
                    finally:
                        sys.stdout = old_stdout

            # GPU ì‚¬ìš©ëŸ‰ ê·¹ëŒ€í™”ë¥¼ ìœ„í•œ ì„¤ì •
            with suppress_stdout():
                model = Llama(
                    model_path = self.model_path,       # GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
                    n_gpu_layers = -1,                  # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUì— ë¡œë“œ
                    main_gpu = 1,                       # 1ë²ˆ GPU ì‚¬ìš© (office ì„œë¹„ìŠ¤ìš©)
                    rope_scaling_type = 2,              # RoPE ìŠ¤ì¼€ì¼ë§ ë°©ì‹ (2 = linear) 
                    rope_freq_scale = 2.0,              # RoPE ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ â†’ ê¸´ ë¬¸ë§¥ ì§€ì›   
                    n_ctx = 8191,                       # ìµœëŒ€ context length
                    n_batch = 2048,                     # ë°°ì¹˜ í¬ê¸° (VRAM ì œí•œ ê³ ë ¤í•œ ì¤‘ê°„ ê°’)
                    verbose = False,                    # ë””ë²„ê¹… ë¡œê·¸ ë¹„í™œì„±í™”  
                    offload_kqv = True,                 # K/Q/V ìºì‹œë¥¼ CPUë¡œ ì˜¤í”„ë¡œë“œí•˜ì—¬ VRAM ì ˆì•½
                    use_mmap = False,                   # ë©”ëª¨ë¦¬ ë§¤í•‘ ë¹„í™œì„±í™” 
                    use_mlock = True,                   # ë©”ëª¨ë¦¬ ì ê¸ˆìœ¼ë¡œ ë©”ëª¨ë¦¬ í˜ì´ì§€ ìŠ¤ì™‘ ë°©ì§€
                    n_threads = 12,                     # CPU ìŠ¤ë ˆë“œ ìˆ˜ (ì½”ì–´ 12ê°œ ê¸°ì¤€ ì ì ˆí•œ ê°’)
                    tensor_split = [1.0],               # ë‹¨ì¼ GPUì—ì„œ ëª¨ë“  í…ì„œ ë¡œë”©
                    split_mode = 1,                     # í…ì„œ ë¶„í•  ë°©ì‹ (1 = ê· ë“± ë¶„í• )
                    flash_attn = True,                  # FlashAttention ì‚¬ìš© (ì†ë„ í–¥ìƒ)
                    cont_batching = True,               # ì—°ì† ë°°ì¹­ í™œì„±í™” (ë©€í‹° ì‚¬ìš©ì ì²˜ë¦¬ì— íš¨ìœ¨ì )
                    numa = False,                       # NUMA ë¹„í™œì„±í™” (ë‹¨ì¼ GPU ì‹œìŠ¤í…œì—ì„œ ë¶ˆí•„ìš”)
                    f16_kv = True,                      # 16bit KV ìºì‹œ ì‚¬ìš©
                    logits_all = False,                 # ë§ˆì§€ë§‰ í† í°ë§Œ logits ê³„ì‚°
                    embedding = False,                  # ì„ë² ë”© ë¹„í™œì„±í™”
                )
            return model
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise e

    def _stream_completion(self, config: BaseConfig.LlamaGenerationConfig) -> None:
        """
        ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ì–´ ì‘ë‹µì„ íì— ë„£ëŠ” ë©”ì„œë“œ (ìµœì í™”)

        Args:
            config (BaseConfig.LlamaGenerationConfig): ìƒì„± íŒŒë¼ë¯¸í„° ê°ì²´
        """
        try:
            stream = self.model.create_completion(
                prompt = config.prompt,
                max_tokens = config.max_tokens,
                temperature = config.temperature,
                top_p = config.top_p,
                min_p = config.min_p,
                typical_p = config.typical_p,
                tfs_z = config.tfs_z,
                repeat_penalty = config.repeat_penalty,
                frequency_penalty = config.frequency_penalty,
                presence_penalty = config.presence_penalty,
                stop = config.stop or ["<|eot_id|>"],
                stream = True,
                seed = config.seed,
            )
            
            token_count = 0
            for output in stream:
                if 'choices' in output and len(output['choices']) > 0:
                    text = output['choices'][0].get('text', '')
                    if text:
                        self.response_queue.put(text)
                        token_count += 1
                        
            print(f"    ìƒì„±ëœ í† í° ìˆ˜: {token_count}")
            self.response_queue.put(None)  # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
            
        except Exception as e:
            print(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.response_queue.put(None)

    def create_streaming_completion(self, config: BaseConfig.LlamaGenerationConfig) -> Generator[str, None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±

        Args:
            config (BaseConfig.LlamaGenerationConfig): ìƒì„± íŒŒë¼ë¯¸í„° ê°ì²´

        Returns:
            Generator[str, None, None]: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ë°˜í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„°
        """
        # í ì´ˆê¸°í™” (ì´ì „ ì‘ë‹µì´ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŒ)
        while not self.response_queue.empty():
            self.response_queue.get()
            
        # ìŠ¤íŠ¸ë¦¬ë° ìŠ¤ë ˆë“œ ì‹œì‘
        thread = Thread(
            target = self._stream_completion,
            args = (config,)
        )
        thread.start()

        # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        token_count = 0
        while True:
            text = self.response_queue.get()
            if text is None:  # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
                break
            token_count += 1
            yield text
            
        # ìŠ¤ë ˆë“œê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        thread.join()
        print(f"    ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {token_count}ê°œ í† í° ìˆ˜ì‹ ")

    def create_completion(self, config: BaseConfig.LlamaGenerationConfig) -> str:
        """
        ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±

        Args:
            config (BaseConfig.LlamaGenerationConfig): ìƒì„± íŒŒë¼ë¯¸í„° ê°ì²´

        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì‘ë‹µ
        """
        try:
            output = self.model.create_completion(
                prompt = config.prompt,
                max_tokens = config.max_tokens,
                temperature = config.temperature,
                top_p = config.top_p,
                stop = config.stop or ["<|eot_id|>"]
            )
            return output['choices'][0]['text'].strip()
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

    def generate_response(self, input_text: str, search_text: str, chat_list: List[Dict]) -> str:
        """
        ì™¸ë¶€ì—ì„œ ì œê³µë°›ì€ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ì‘ë‹µ ìƒì„± ë©”ì„œë“œ

        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            search_text (str): ì™¸ë¶€ì—ì„œ ì œê³µë°›ì€ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
            chat_list (List[Dict]): ëŒ€í™” ê¸°ë¡

        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        start_time = time.time()
        try:
            # ì™¸ë¶€ì—ì„œ ì œê³µë°›ì€ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
            if search_text:
                print(f"    âœ… ì™¸ë¶€ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ í™œìš©: {len(search_text)} ë¬¸ì")
                reference_text = search_text
            else:
                # ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì •ë³´ ì‚¬ìš©
                current_time = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
                reference_text = f"í˜„ì¬ ì‹œê°„ì€ {current_time}ì…ë‹ˆë‹¤.\n\n"
                print(f"    âš ï¸ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ, ê¸°ë³¸ ì •ë³´ ì‚¬ìš©")

            # ëŒ€í™” ê¸°ë¡ ì •ê·œí™” - MongoDB êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
            normalized_chat_list = []
            if chat_list and len(chat_list) > 0:
                for chat in chat_list:
                    # MongoDBì—ì„œ ê°€ì ¸ì˜¨ ë©”ì‹œì§€ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                    normalized_chat = {
                        "index": chat.get("message_idx", chat.get("index")),
                        "input_data": chat.get("content", chat.get("input_data", "")),
                        "output_data": self._normalize_escape_chars(
                            chat.get("answer", chat.get("output_data", ""))
                        )
                    }
                    normalized_chat_list.append(normalized_chat)
            else:
                normalized_chat_list = []

            # ìºë¦­í„° ì •ë³´ ì„¤ì •
            self.character_info = BaseConfig.OfficePrompt(
                name = self.data.get("character_name", "AI Assistant"),
                context = self.data.get("character_setting", "Helpful AI assistant"),
                reference_data = reference_text,
                user_input = input_text,
                chat_list = normalized_chat_list,
            )

            # RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = build_llama3_prompt(
                character_info=self.character_info,
                vector_context=reference_text
            )
            
            # ê· í˜• ì¡íŒ ì„¤ì •ìœ¼ë¡œ ìˆ˜ì •
            self.config = BaseConfig.LlamaGenerationConfig(
                prompt = prompt,
                max_tokens = 1024,                  # ì ì ˆí•œ í† í° ìˆ˜
                temperature = 0.7,                  # ì˜¨ë„ ì ì ˆíˆ ì¡°ì •
                top_p = 0.9,                        # top_p ë³µì›
                min_p = 0.1,                        # min_p ë³µì›
                typical_p = 1.0,                    # typical_p ì¶”ê°€
                tfs_z = 1.1,                        # tfs_z ë³µì›
                repeat_penalty = 1.08,              # repeat_penalty ë³µì›
                frequency_penalty = 0.1,            # frequency_penalty ë³µì›
                presence_penalty = 0.1,             # presence_penalty ë³µì›
                seed = None,                        # ì‹œë“œ ì—†ìŒ (ë‹¤ì–‘ì„± í™•ë³´)
            )
            
            print(f"    ğŸš€ ì‘ë‹µ ìƒì„± ì‹œì‘...")
            chunks = []
            for text_chunk in self.create_streaming_completion(config = self.config):
                chunks.append(text_chunk)
            
            result = "".join(chunks)
            generation_time = time.time() - start_time
            print(f"    âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì†Œìš” ì‹œê°„: {generation_time:.2f}ì´ˆ)")
            return result

        except Exception as e:
            generation_time = time.time() - start_time
            print(f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} (ì†Œìš” ì‹œê°„: {generation_time:.2f}ì´ˆ)")
            return f"ì˜¤ë¥˜: {str(e)}"

    def _normalize_escape_chars(self, text: str) -> str:
        """
        ì´ìŠ¤ì¼€ì´í”„ ë¬¸ìê°€ ì¤‘ë³µëœ ë¬¸ìì—´ì„ ì •ê·œí™”í•©ë‹ˆë‹¤
        """
        if not text:
            return ""
            
        # ì´ìŠ¤ì¼€ì´í”„ëœ ê°œí–‰ë¬¸ì ë“±ì„ ì •ê·œí™”
        result = text.replace("\\n", "\n")
        result = result.replace("\\\\n", "\n")
        result = result.replace('\\"', '"')
        result = result.replace("\\\\", "\\")
        
        return result
