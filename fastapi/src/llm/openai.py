'''
íŒŒì¼ì€ OpenAIModel í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê³  OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬,
GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
ChromaDBëŠ” LangChainìœ¼ë¡œ ì—°ê²°í•˜ê³ , ëª¨ë¸ì€ OpenAI APIë¡œ ì„œë¹™í•©ë‹ˆë‹¤.
'''
from typing import  Generator, List, Dict
import os
import json
import textwrap
import time
from openai import OpenAI
from pathlib import Path
from queue import Queue
from threading import Thread
from datetime import datetime

from dotenv import load_dotenv
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.schema.retriever import BaseRetriever
from langchain.callbacks.manager import CallbackManagerForRetrieverRun
from pydantic import Field

from service.vector_client import VectorSearchHandler
from domain import BaseConfig
from core import app_state as AppState

GREEN = "\033[32m"
RED = "\033[31m"
YELLOW = "\033[33m"
BLUE = "\033[34m"
RESET = "\033[0m"

class VectorRetriever(BaseRetriever):
    """
    VectorSearchHandlerë¥¼ LangChain Retrieverë¡œ ë˜í•‘ (ChromaDB ì—°ê²°ìš©)
    """
    vector_handler: VectorSearchHandler = Field(description="ChromaDB ë²¡í„° ê²€ìƒ‰ í•¸ë“¤ëŸ¬")
    
    class Config:
        arbitrary_types_allowed = True
        
    def __init__(self, vector_handler: VectorSearchHandler, **kwargs):
        super().__init__(vector_handler=vector_handler, **kwargs)
    
    def _get_relevant_documents(
        self, 
        query: str, 
        *, 
        run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """
        VectorSearchHandlerì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ LangChain Documentë¡œ ë³€í™˜
        """
        if not self.vector_handler:
            print("    âš ï¸ VectorSearchHandlerê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            print(f"    ğŸ” ChromaDB ë²¡í„° ê²€ìƒ‰ ì¤‘: '{query[:50]}...'")
            
            # ë§ë­‰ì¹˜ ë°ì´í„° ê²€ìƒ‰
            corpus_results = self.vector_handler.search_relevant_documents(
                query=query,
                n_results=3,
                source_type="corpus"
            )
            
            # Q&A ë°ì´í„° ê²€ìƒ‰
            qa_results = self.vector_handler.search_relevant_documents(
                query=query,
                n_results=2,
                source_type="qa_answer"
            )
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            all_results = corpus_results + qa_results
            
            print(f"    ğŸ“„ {len(all_results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨ (ë§ë­‰ì¹˜: {len(corpus_results)}, Q&A: {len(qa_results)})")
            
            # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            documents = []
            for result in all_results:
                doc = Document(
                    page_content=result.get('content', ''),
                    metadata={
                        **result.get('metadata', {}),
                        'similarity': result.get('similarity', 0),
                        'rank': result.get('rank', 0)
                    }
                )
                documents.append(doc)
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            documents.sort(key=lambda x: x.metadata.get('similarity', 0), reverse=True)
            
            return documents[:5]
            
        except Exception as e:
            print(f"    âŒ ChromaDB ë²¡í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

def build_rag_prompt_template() -> PromptTemplate:
    """
    RAGë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± (LangChain í…œí”Œë¦¿ í˜•ì‹)
    """
    template = textwrap.dedent("""
        ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ìƒë‹´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì•„ë˜ ChromaDBì—ì„œ ê²€ìƒ‰ëœ ì˜ë£Œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

        ê²€ìƒ‰ëœ ì˜ë£Œ ì •ë³´:
        {context}

        ëŒ€í™” ê¸°ë¡:
        {chat_history}

        ì§€ì‹œ ì‚¬í•­:
        - í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
        - ê²€ìƒ‰ëœ ì˜ë£Œ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”
        - ì˜ë£Œ ì •ë³´ëŠ” ì •í™•í•˜ê³  ì‹ ì¤‘í•˜ê²Œ ì œê³µí•˜ë©°, ì‘ê¸‰ìƒí™©ì´ë‚˜ ì‹¬ê°í•œ ì¦ìƒì˜ ê²½ìš° ì¦‰ì‹œ ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œìœ í•˜ì„¸ìš”
        - ê²€ìƒ‰ëœ ì •ë³´ê°€ ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ì—†ë‹¤ë©´ ì¼ë°˜ì ì¸ ì˜ë£Œ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
        - ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ë„ë¡ í•˜ì„¸ìš”

        ì‚¬ìš©ì ì§ˆë¬¸: {question}

        ë‹µë³€:
    """).strip()
    
    return PromptTemplate(
        template=template,
        input_variables=["context", "chat_history", "question"]
    )

class OpenAIModel:
    """
    OpenAI APIë¡œ ëª¨ë¸ ì„œë¹™ + LangChainìœ¼ë¡œ ChromaDB ì—°ê²°í•˜ëŠ” RAG ì‹œìŠ¤í…œ
    """
    def __init__(self, model_id: str = "gpt-4.1") -> None:
        """
        OpenAIModel í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì†Œë“œ
        
        Args:
            model_id (str): ì‚¬ìš©í•  OpenAI ëª¨ë¸ ID (ê¸°ë³¸ê°’: gpt-4.1)
        """
        self.model_id = model_id
        self.file_path = "/app/prompt/config-OpenAI.json"
        self.loading_text = f"{BLUE}LOADING{RESET}:    {self.model_id} ë¡œë“œ ì¤‘..."
        
        print("\n"+ f"{BLUE}LOADING{RESET}:  " + "="*len(self.loading_text))
        print(f"{BLUE}LOADING{RESET}:    {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")

        # í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ë¡œë“œ
        self._load_api_key()

        # JSON íŒŒì¼ ì½ê¸° (íŒŒì¼ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if os.path.exists(self.file_path):
            with open(self.file_path, 'r', encoding='utf-8') as file:
                self.data: BaseConfig.BaseConfig = json.load(file)
        else:
            # ê¸°ë³¸ ì„¤ì •
            self.data = {
                "character_name": "Dogi AI",
                "greeting": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Dogi AIì…ë‹ˆë‹¤.",
                "character_setting": ["ì „ë¬¸ì ì¸ ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ìƒë‹´ AI ì–´ì‹œìŠ¤í„´íŠ¸"]
            }

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        print(f"{BLUE}LOADING{RESET}:    OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
        self.client = OpenAI(api_key=self.api_key)
        print(f"{BLUE}LOADING{RESET}:    OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        
        # ChromaDB + LangChain RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_rag_components()
        
        print(f"{BLUE}LOADING{RESET}:  " + "="*len(self.loading_text) + "\n")
        
        self.response_queue: Queue = Queue()

    def _load_api_key(self):
        """
        .env íŒŒì¼ì—ì„œ OpenAI API í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        try:
            # .env íŒŒì¼ ê²½ë¡œ ì„¤ì • (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼)
            env_file_path = Path(__file__).resolve().parents[1] / ".env"
            load_dotenv(env_file_path)
            
            self.api_key = os.getenv("OPENAI_API_KEY")
            
            if not self.api_key:
                raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            print(f"{BLUE}LOADING{RESET}:    OpenAI API í‚¤ ë¡œë“œ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"{RED}ERROR{RESET}:     OpenAI API í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e

    def _initialize_rag_components(self):
        """
        ChromaDB ì—°ê²° + LangChain RAG ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”
        """
        try:
            # VectorSearchHandler ì´ˆê¸°í™” (app_stateì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            self.vector_handler = AppState.get_vector_handler()
            
            if self.vector_handler and self.vector_handler.health_check():
                print(f"{BLUE}LOADING{RESET}:    ChromaDB ì—°ê²° í™•ì¸ ì™„ë£Œ!")
                
                # LangChain Retriever ì´ˆê¸°í™” (ChromaDB ì—°ê²°ìš©)
                self.retriever = VectorRetriever(vector_handler=self.vector_handler)
                
                # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
                self.prompt_template = build_rag_prompt_template()
                
                self.rag_available = True
                print(f"{BLUE}LOADING{RESET}:    LangChain RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ!")
                
            else:
                print(f"{YELLOW}WARNING{RESET}:  ChromaDB ì—°ê²° ì‹¤íŒ¨, RAG ê¸°ëŠ¥ ì œí•œë¨")
                self.retriever = None
                self.prompt_template = None
                self.rag_available = False
                
        except Exception as e:
            print(f"{RED}ERROR{RESET}:     RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.retriever = None
            self.prompt_template = None
            self.rag_available = False

    def _format_documents(self, docs: List[Document]) -> str:
        """
        LangChain Documentë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
        """
        if not docs:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_docs = []
        corpus_docs = []
        qa_docs = []
        
        # ë¬¸ì„œ íƒ€ì…ë³„ë¡œ ë¶„ë¥˜
        for doc in docs:
            source_type = doc.metadata.get('source_type', '')
            if source_type == 'corpus':
                corpus_docs.append(doc)
            elif source_type in ['qa_answer', 'qa_question']:
                qa_docs.append(doc)
            else:
                corpus_docs.append(doc)
        
        # ë§ë­‰ì¹˜ ë¬¸ì„œ í¬ë§·íŒ…
        if corpus_docs:
            formatted_docs.append("=== ê´€ë ¨ ì˜ë£Œ ì •ë³´ (ë§ë­‰ì¹˜ ë°ì´í„°) ===")
            for i, doc in enumerate(corpus_docs, 1):
                content = doc.page_content
                metadata = doc.metadata
                
                doc_info = f"[ë¬¸ì„œ {i}]"
                if metadata.get('department'):
                    doc_info += f" (ì§„ë£Œê³¼: {metadata['department']})"
                if metadata.get('similarity'):
                    doc_info += f" (ìœ ì‚¬ë„: {metadata['similarity']:.3f})"
                
                formatted_docs.append(f"{doc_info}\n{content}")
        
        # Q&A ë¬¸ì„œ í¬ë§·íŒ…
        if qa_docs:
            formatted_docs.append("\n=== ê´€ë ¨ ì§ˆì˜ì‘ë‹µ (Q&A ë°ì´í„°) ===")
            for i, doc in enumerate(qa_docs, 1):
                content = doc.page_content
                metadata = doc.metadata
                
                doc_info = f"[Q&A {i}]"
                if metadata.get('department'):
                    doc_info += f" (ì§„ë£Œê³¼: {metadata['department']})"
                if metadata.get('similarity'):
                    doc_info += f" (ìœ ì‚¬ë„: {metadata['similarity']:.3f})"
                
                formatted_docs.append(f"{doc_info}\n{content}")
        
        return "\n\n".join(formatted_docs)

    def _format_chat_history(self, chat_list: List[Dict]) -> str:
        """
        ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
        """
        if not chat_list:
            return "ì´ì „ ëŒ€í™” ì—†ìŒ"
        
        formatted_history = []
        for chat in chat_list[-3:]:  # ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ í¬í•¨
            user_msg = chat.get("content", chat.get("input_data", ""))
            ai_msg = chat.get("answer", chat.get("output_data", ""))
            
            if user_msg:
                formatted_history.append(f"ì‚¬ìš©ì: {user_msg}")
            if ai_msg:
                formatted_history.append(f"AI: {ai_msg}")
        
        return "\n".join(formatted_history)

    def _convert_chat_to_messages(self, chat_list: List[Dict], current_input: str, context: str = None) -> List[Dict]:
        """
        ëŒ€í™” ê¸°ë¡ì„ OpenAI API ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        """
        messages = []
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
        current_time = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
        
        system_content = textwrap.dedent(f"""
            ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ìƒë‹´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
            í˜„ì¬ ì‹œê°„: {current_time}

            ì§€ì‹œ ì‚¬í•­:
            - í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
            - ì˜ë£Œ ì •ë³´ëŠ” ì •í™•í•˜ê³  ì‹ ì¤‘í•˜ê²Œ ì œê³µí•˜ë©°, ì‘ê¸‰ìƒí™©ì´ë‚˜ ì‹¬ê°í•œ ì¦ìƒì˜ ê²½ìš° ì¦‰ì‹œ ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œìœ í•˜ì„¸ìš”
            - ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ë„ë¡ í•˜ì„¸ìš”
        """).strip()

        if context:
            system_content += f"\n\nì°¸ê³ í•  ì˜ë£Œ ì •ë³´:\n{context}"

        messages.append({
            "role": "system",
            "content": system_content
        })
        
        # ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (ìµœê·¼ 5ê°œë§Œ)
        for chat in chat_list[-5:]:
            user_msg = chat.get("content", chat.get("input_data", ""))
            ai_msg = chat.get("answer", chat.get("output_data", ""))
            
            if user_msg:
                messages.append({
                    "role": "user",
                    "content": user_msg
                })
            if ai_msg:
                messages.append({
                    "role": "assistant",
                    "content": ai_msg
                })
        
        # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
        messages.append({
            "role": "user",
            "content": current_input
        })
        
        return messages

    def generate_response(self, input_text: str, chat_list: List[Dict]) -> str:
        """
        ChromaDB RAG + OpenAI APIë¥¼ í™œìš©í•œ ì‘ë‹µ ìƒì„±

        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            chat_list (List[Dict]): ëŒ€í™” ê¸°ë¡

        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        chunks = []
        for chunk in self.generate_response_stream(input_text, chat_list):
            chunks.append(chunk)
        return "".join(chunks)

    def generate_response_stream(self, input_text: str, chat_list: List[Dict]) -> Generator[str, None, None]:
        """
        ChromaDB RAG + OpenAI APIë¥¼ í™œìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±

        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            chat_list (List[Dict]): ëŒ€í™” ê¸°ë¡

        Yields:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤
        """
        start_time = time.time()
        try:
            print(f"    ğŸš€ ChromaDB RAG + OpenAI API ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹œì‘...")
            
            # RAG ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if self.rag_available and self.retriever:
                # LangChain Retrieverë¡œ ChromaDBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
                docs = self.retriever.get_relevant_documents(input_text)
                context = self._format_documents(docs)
                
                print(f"    ğŸ” ChromaDB RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘...")
                
                # OpenAI API ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                messages = self._convert_chat_to_messages(chat_list, input_text, context)
                
                # OpenAI API ì„¤ì •
                config = BaseConfig.OpenAIGenerationConfig(
                    messages=messages,
                    max_tokens=4096,
                    temperature=1.0,
                )
                
                # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
                for text_chunk in self.create_streaming_completion(config):
                    yield text_chunk
                    
                generation_time = time.time() - start_time
                print(f"    âœ… ChromaDB RAG + OpenAI API ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ (ì†Œìš” ì‹œê°„: {generation_time:.2f}ì´ˆ)")
                
            else:
                print(f"    âš ï¸ ChromaDB RAG ê¸°ëŠ¥ ì‚¬ìš© ë¶ˆê°€, OpenAI API ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜")
                # í´ë°± ìŠ¤íŠ¸ë¦¬ë° (ìˆœìˆ˜ OpenAI API)
                for text_chunk in self._generate_fallback_response_stream(input_text, chat_list):
                    yield text_chunk

        except Exception as e:
            generation_time = time.time() - start_time
            print(f"âŒ ChromaDB RAG ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} (ì†Œìš” ì‹œê°„: {generation_time:.2f}ì´ˆ)")
            # ì—ëŸ¬ ì‹œ í´ë°± ìŠ¤íŠ¸ë¦¬ë°
            for text_chunk in self._generate_fallback_response_stream(input_text, chat_list):
                yield text_chunk

    def _generate_fallback_response_stream(self, input_text: str, chat_list: List[Dict]) -> Generator[str, None, None]:
        """
        ChromaDB RAG ì‹¤íŒ¨ì‹œ ìˆœìˆ˜ OpenAI APIë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        """
        try:
            print(f"    ğŸ”„ ìˆœìˆ˜ OpenAI API ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‘ë‹µ ìƒì„±...")
            
            # OpenAI API ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            messages = self._convert_chat_to_messages(chat_list, input_text)
            
            # OpenAI API ì„¤ì •
            config = BaseConfig.OpenAIGenerationConfig(
                messages=messages,
                max_tokens=4096,
                temperature=1.0,
            )
            
            # OpenAI API ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
            for text_chunk in self.create_streaming_completion(config):
                yield text_chunk
            
        except Exception as e:
            print(f"âŒ ìˆœìˆ˜ OpenAI API ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _stream_completion(self, config: BaseConfig.OpenAIGenerationConfig) -> None:
        """
        OpenAI Responses APIë¡œ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ì–´ ì‘ë‹µì„ íì— ë„£ëŠ” ë©”ì„œë“œ (ìŠ¤íŠ¸ë¦¬ë°ìš©)
        """
        try:
            # Responses API ìŠ¤íŠ¸ë¦¬ë° ì‹œë„
            stream = self.client.responses.create(
                model=self.model_id,
                input=config.messages,
                max_output_tokens=config.max_tokens,
                temperature=config.temperature,
                stream=True,
            )

            token_count = 0
            for event in stream:
                # í…ìŠ¤íŠ¸ ë¸íƒ€ ì´ë²¤íŠ¸ë§Œ ì „ì†¡
                if getattr(event, "type", None) == "response.output_text.delta":
                    delta = getattr(event, "delta", None)
                    if delta:
                        self.response_queue.put(delta)
                        token_count += 1
                elif getattr(event, "type", None) in ("response.completed", "response.error"):
                    break

            print(f"    OpenAI API ìƒì„±ëœ í† í° ìˆ˜: {token_count}")
            self.response_queue.put(None)

        except Exception as e:
            err_msg = str(e)
            print(f"OpenAI API ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            try:
                resp = self.client.responses.create(
                    model=self.model_id,
                    input=config.messages,
                    max_output_tokens=config.max_tokens,
                    temperature=config.temperature,
                )
                # í¸ì˜ í”„ë¡œí¼í‹° (SDKê°€ ì œê³µ)
                content = getattr(resp, "output_text", None)
                if not content:
                    # êµ¬ì¡°í˜• ì‘ë‹µ í•©ì¹˜ê¸° (ë³´ìˆ˜ì  ì²˜ë¦¬)
                    content_parts = []
                    for out in getattr(resp, "output", []) or []:
                        for c in getattr(out, "content", []) or []:
                            text = getattr(getattr(c, "text", None), "value", None)
                            if text:
                                content_parts.append(text)
                    content = "".join(content_parts)
                if content:
                    self.response_queue.put(content)
                self.response_queue.put(None)
            except Exception as e2:
                print(f"OpenAI API ë¹„ìŠ¤íŠ¸ë¦¬ë° ì¬ì‹œë„ ì‹¤íŒ¨: {e2}")
                self.response_queue.put("ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸ë¡œëŠ” ìŠ¤íŠ¸ë¦¬ë°ì´ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                self.response_queue.put(None)

    def create_streaming_completion(self, config: BaseConfig.OpenAIGenerationConfig) -> Generator[str, None, None]:
        """
        OpenAI APIë¡œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±
        """
        # í ì´ˆê¸°í™”
        while not self.response_queue.empty():
            self.response_queue.get()
            
        # OpenAI API ìŠ¤íŠ¸ë¦¬ë° ìŠ¤ë ˆë“œ ì‹œì‘
        thread = Thread(
            target=self._stream_completion,
            args=(config,)
        )
        thread.start()

        # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        token_count = 0
        while True:
            text = self.response_queue.get()
            if text is None:
                break
            token_count += 1
            yield text
            
        # ìŠ¤ë ˆë“œê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        thread.join()
        print(f"    OpenAI API ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {token_count}ê°œ í† í° ìˆ˜ì‹ ")