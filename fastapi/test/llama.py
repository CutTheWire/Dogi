'''
íŒŒì¼ì€ LlamaModel, BaseConfig.OfficePrompt í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê³  llama_cpp_cudaë¥¼ ì‚¬ìš©í•˜ì—¬,
Meta-Llama-3.1-8B-Claude.Q4_0.gguf ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
RAG(Retrieval-Augmented Generation) ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.
'''
from typing import Optional, Generator, List, Dict
from llama_cpp_cuda import Llama

import os
import sys
import json
import warnings
import time
from queue import Queue
from threading import Thread
from contextlib import contextmanager
from datetime import datetime

from src.domain import BaseConfig, VectorClient

GREEN = "\033[32m"
RED = "\033[31m"
YELLOW = "\033[33m"
BLUE = "\033[34m"
RESET = "\033[0m"

def build_llama3_prompt_with_rag(character_info: BaseConfig.OfficePrompt, 
                                vector_context: str = "") -> str:
    """
    RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ Llama3 GGUF í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        character_info (BaseConfig.OfficePrompt): ìºë¦­í„° ê¸°ë³¸ ì •ë³´ ë° ëŒ€í™” ë§¥ë½ í¬í•¨ ê°ì²´
        vector_context (str): ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì–»ì€ ê´€ë ¨ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸

    Returns:
        str: Llama3 GGUF í¬ë§·ìš© í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    # ë²¡í„° ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ì™€ ê¸°ì¡´ ì°¸ê³  ì •ë³´ ê²°í•©
    reference_data = vector_context if vector_context else character_info.reference_data
    
    system_prompt = (
        f"ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ {character_info.name}ì…ë‹ˆë‹¤.\n"
        f"ë‹¹ì‹ ì˜ ì—­í• : {character_info.context}\n\n"
        f"ì°¸ê³  ì •ë³´ (ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆì„ ê²½ìš°ì—ë§Œ í™œìš©í•˜ì„¸ìš”):\n"
        f"{reference_data}\n\n"
        f"ì§€ì‹œ ì‚¬í•­:\n"
        f"- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”\n"
        f"- ì¹œì ˆí•˜ê³  ìœ ìµí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”\n"
        f"- ìœ„ì˜ ì°¸ê³  ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ë˜, ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ì •ë³´ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”\n"
        f"- ì˜ë£Œ ì •ë³´ëŠ” ì •í™•í•˜ê³  ì‹ ì¤‘í•˜ê²Œ ì œê³µí•˜ë©°, ì „ë¬¸ì˜ ìƒë‹´ì„ ê¶Œìœ í•˜ì„¸ìš”\n"
        f"- ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ë„ë¡ í•˜ì„¸ìš”\n"
    )

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‹œì‘
    prompt = (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        f"{system_prompt}<|eot_id|>"
    )

    # ëŒ€í™” ê¸°ë¡ ì¶”ê°€
    if character_info.chat_list:
        for chat in character_info.chat_list:
            user_input = chat.get("input_data", "")
            assistant_output = chat.get("output_data", "")

            if user_input:
                prompt += (
                    "<|start_header_id|>user<|end_header_id|>\n"
                    f"{user_input}<|eot_id|>"
                )
            if assistant_output:
                prompt += (
                    "<|start_header_id|>assistant<|end_header_id|>\n"
                    f"{assistant_output}<|eot_id|>"
                )

    # ìµœì‹  ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    prompt += (
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{character_info.user_input}<|eot_id|>"
        "<|start_header_id|>assistant<|end_header_id|>\n"
    )

    return prompt

def build_llama3_prompt(character_info: BaseConfig.OfficePrompt) -> str:
    """
    ìºë¦­í„° ì •ë³´ì™€ ëŒ€í™” ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ Llama3 GGUF í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        character_info (BaseConfig.OfficePrompt): ìºë¦­í„° ê¸°ë³¸ ì •ë³´ ë° ëŒ€í™” ë§¥ë½ í¬í•¨ ê°ì²´

    Returns:
        str: Llama3 GGUF í¬ë§·ìš© í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    system_prompt = (
        f"ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ {character_info.name}ì…ë‹ˆë‹¤.\n"
        f"ë‹¹ì‹ ì˜ ì—­í• : {character_info.context}\n\n"
        f"ì°¸ê³  ì •ë³´ (ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆì„ ê²½ìš°ì—ë§Œ í™œìš©í•˜ì„¸ìš”):\n"
        f"{character_info.reference_data}\n\n"
        f"ì§€ì‹œ ì‚¬í•­:\n"
        f"- í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”\n"
        f"- ì¹œì ˆí•˜ê³  ìœ ìµí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”\n"
        f"- ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ì°¸ê³  ì •ë³´ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”\n"
        f"- ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ë„ë¡ í•˜ì„¸ìš”\n"
    )

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‹œì‘
    prompt = (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        f"{system_prompt}<|eot_id|>"
    )

    # ëŒ€í™” ê¸°ë¡ ì¶”ê°€
    if character_info.chat_list:
        for chat in character_info.chat_list:
            user_input = chat.get("input_data", "")
            assistant_output = chat.get("output_data", "")

            if user_input:
                prompt += (
                    "<|start_header_id|>user<|end_header_id|>\n"
                    f"{user_input}<|eot_id|>"
                )
            if assistant_output:
                prompt += (
                    "<|start_header_id|>assistant<|end_header_id|>\n"
                    f"{assistant_output}<|eot_id|>"
                )

    # ìµœì‹  ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    prompt += (
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{character_info.user_input}<|eot_id|>"
        "<|start_header_id|>assistant<|end_header_id|>\n"
    )

    return prompt

class LlamaModel:
    """
    RAG ê¸°ëŠ¥ì„ ê°–ì¶˜ GGUF í¬ë§· llama-3-Korean-Bllossom-8B ëª¨ë¸ í´ë˜ìŠ¤
    ë²¡í„° ê²€ìƒ‰ì„ í†µí•´ ê´€ë ¨ ì˜ë£Œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def __init__(self, enable_rag: bool = True) -> None:
        """
        LlamaModel í´ë˜ìŠ¤ ì´ˆê¸°í™” ë©”ì†Œë“œ
        
        Args:
            enable_rag: RAG ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
        """
        self.model_id = 'llama-3-Korean-Bllossom-8B'
        self.model_path = "./fastapi/models/llama-3-Korean-Bllossom-8B.gguf"
        self.file_path = './fastapi/prompt/config-Llama.json'
        self.loading_text = f"{BLUE}LOADING{RESET}:    {self.model_id} ë¡œë“œ ì¤‘..."
        self.character_info: Optional[BaseConfig.OfficePrompt] = None
        self.config: Optional[BaseConfig.LlamaGenerationConfig] = None
        self.enable_rag = enable_rag
        
        print("\n"+ f"{BLUE}LOADING{RESET}:  " + "="*len(self.loading_text))
        print(f"{BLUE}LOADING{RESET}:    {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")

        # JSON íŒŒì¼ ì½ê¸°
        with open(self.file_path, 'r', encoding = 'utf-8') as file:
            self.data: BaseConfig.BaseConfig = json.load(file)

        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if self.enable_rag:
            print(f"{BLUE}LOADING{RESET}:    RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            try:
                self.vector_search = VectorClient.VectorSearchHandler()
                if self.vector_search.health_check():
                    print(f"{GREEN}SUCCESS{RESET}:   RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
                else:
                    print(f"{YELLOW}WARNING{RESET}:  RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, RAG ë¹„í™œì„±í™”")
                    self.enable_rag = False
                    self.vector_search = None
            except Exception as e:
                print(f"{YELLOW}WARNING{RESET}:  RAG ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}, RAG ë¹„í™œì„±í™”")
                self.enable_rag = False
                self.vector_search = None
        else:
            self.vector_search = None

        # ì§„í–‰ ìƒíƒœ í‘œì‹œ
        print(f"{BLUE}LOADING{RESET}:    {__class__.__name__} ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        self.model: Llama = self._load_model()
        print(f"{BLUE}LOADING{RESET}:    ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print(f"{BLUE}LOADING{RESET}:  " + "="*len(self.loading_text) + "\n")
        
        self.response_queue: Queue = Queue()

    def _load_model(self) -> Llama:
        """
        GGUF í¬ë§·ì˜ Llama ëª¨ë¸ì„ ë¡œë“œí•˜ê³  GPU ê°€ì†ì„ ìµœëŒ€í™”í•©ë‹ˆë‹¤.
        """
        print(f"{self.loading_text}")
        try:
            warnings.filterwarnings("ignore")
            
            @contextmanager
            def suppress_stdout():
                with open(os.devnull, "w") as devnull:
                    old_stdout = sys.stdout
                    sys.stdout = devnull
                    try:
                        yield
                    finally:
                        sys.stdout = old_stdout

            # GPU ì‚¬ìš©ëŸ‰ ê·¹ëŒ€í™”ë¥¼ ìœ„í•œ ì„¤ì •
            with suppress_stdout():
                model = Llama(
                    model_path = self.model_path,       # GGUF ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
                    n_gpu_layers = -1,                  # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUì— ë¡œë“œ
                    main_gpu = 1,                       # 1ë²ˆ GPU ì‚¬ìš© (office ì„œë¹„ìŠ¤ìš©)
                    rope_scaling_type = 2,              # RoPE ìŠ¤ì¼€ì¼ë§ ë°©ì‹ (2 = linear) 
                    rope_freq_scale = 2.0,              # RoPE ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ â†’ ê¸´ ë¬¸ë§¥ ì§€ì›   
                    n_ctx = 8191,                       # ìµœëŒ€ context length
                    n_batch = 2048,                     # ë°°ì¹˜ í¬ê¸° (VRAM ì œí•œ ê³ ë ¤í•œ ì¤‘ê°„ ê°’)
                    verbose = False,                    # ë””ë²„ê¹… ë¡œê·¸ ë¹„í™œì„±í™”  
                    offload_kqv = True,                 # K/Q/V ìºì‹œë¥¼ CPUë¡œ ì˜¤í”„ë¡œë“œí•˜ì—¬ VRAM ì ˆì•½
                    use_mmap = False,                   # ë©”ëª¨ë¦¬ ë§¤í•‘ ë¹„í™œì„±í™” 
                    use_mlock = True,                   # ë©”ëª¨ë¦¬ ì ê¸ˆìœ¼ë¡œ ë©”ëª¨ë¦¬ í˜ì´ì§€ ìŠ¤ì™‘ ë°©ì§€
                    n_threads = 12,                     # CPU ìŠ¤ë ˆë“œ ìˆ˜ (ì½”ì–´ 12ê°œ ê¸°ì¤€ ì ì ˆí•œ ê°’)
                    tensor_split = [1.0],               # ë‹¨ì¼ GPUì—ì„œ ëª¨ë“  í…ì„œ ë¡œë”©
                    split_mode = 1,                     # í…ì„œ ë¶„í•  ë°©ì‹ (1 = ê· ë“± ë¶„í• )
                    flash_attn = True,                  # FlashAttention ì‚¬ìš© (ì†ë„ í–¥ìƒ)
                    cont_batching = True,               # ì—°ì† ë°°ì¹­ í™œì„±í™” (ë©€í‹° ì‚¬ìš©ì ì²˜ë¦¬ì— íš¨ìœ¨ì )
                    numa = False,                       # NUMA ë¹„í™œì„±í™” (ë‹¨ì¼ GPU ì‹œìŠ¤í…œì—ì„œ ë¶ˆí•„ìš”)
                    f16_kv = True,                      # 16bit KV ìºì‹œ ì‚¬ìš©
                    logits_all = False,                 # ë§ˆì§€ë§‰ í† í°ë§Œ logits ê³„ì‚°
                    embedding = False,                  # ì„ë² ë”© ë¹„í™œì„±í™”
                )
            return model
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise e

    def _stream_completion(self, config: BaseConfig.LlamaGenerationConfig) -> None:
        """
        ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ì–´ ì‘ë‹µì„ íì— ë„£ëŠ” ë©”ì„œë“œ (ìµœì í™”)

        Args:
            config (BaseConfig.LlamaGenerationConfig): ìƒì„± íŒŒë¼ë¯¸í„° ê°ì²´
        """
        try:
            stream = self.model.create_completion(
                prompt = config.prompt,
                max_tokens = config.max_tokens,
                temperature = config.temperature,
                top_p = config.top_p,
                min_p = config.min_p,
                typical_p = config.typical_p,
                tfs_z = config.tfs_z,
                repeat_penalty = config.repeat_penalty,
                frequency_penalty = config.frequency_penalty,
                presence_penalty = config.presence_penalty,
                stop = config.stop or ["<|eot_id|>"],
                stream = True,
                seed = config.seed,
            )
            
            token_count = 0
            for output in stream:
                if 'choices' in output and len(output['choices']) > 0:
                    text = output['choices'][0].get('text', '')
                    if text:
                        self.response_queue.put(text)
                        token_count += 1
                        
            print(f"    ìƒì„±ëœ í† í° ìˆ˜: {token_count}")
            self.response_queue.put(None)  # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
            
        except Exception as e:
            print(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.response_queue.put(None)

    def create_streaming_completion(self, config: BaseConfig.LlamaGenerationConfig) -> Generator[str, None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±

        Args:
            config (BaseConfig.LlamaGenerationConfig): ìƒì„± íŒŒë¼ë¯¸í„° ê°ì²´

        Returns:
            Generator[str, None, None]: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ë°˜í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„°
        """
        # í ì´ˆê¸°í™” (ì´ì „ ì‘ë‹µì´ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŒ)
        while not self.response_queue.empty():
            self.response_queue.get()
            
        # ìŠ¤íŠ¸ë¦¬ë° ìŠ¤ë ˆë“œ ì‹œì‘
        thread = Thread(
            target = self._stream_completion,
            args = (config,)
        )
        thread.start()

        # ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        token_count = 0
        while True:
            text = self.response_queue.get()
            if text is None:  # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
                break
            token_count += 1
            yield text
            
        # ìŠ¤ë ˆë“œê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        thread.join()
        print(f"    ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {token_count}ê°œ í† í° ìˆ˜ì‹ ")

    def create_completion(self, config: BaseConfig.LlamaGenerationConfig) -> str:
        """
        ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±

        Args:
            config (BaseConfig.LlamaGenerationConfig): ìƒì„± íŒŒë¼ë¯¸í„° ê°ì²´

        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì‘ë‹µ
        """
        try:
            output = self.model.create_completion(
                prompt = config.prompt,
                max_tokens = config.max_tokens,
                temperature = config.temperature,
                top_p = config.top_p,
                stop = config.stop or ["<|eot_id|>"]
            )
            return output['choices'][0]['text'].strip()
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

    def generate_response(self, input_text: str, search_text: str, chat_list: List[Dict]) -> str:
        """
        RAG ê¸°ëŠ¥ì„ í™œìš©í•œ ìµœì í™”ëœ ì‘ë‹µ ìƒì„± ë©”ì„œë“œ

        Args:
            input_text (str): ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            search_text (str): ê¸°ì¡´ ê²€ìƒ‰ í…ìŠ¤íŠ¸ (RAG í™œì„±í™” ì‹œ ë¬´ì‹œë¨)
            chat_list (List[Dict]): ëŒ€í™” ê¸°ë¡

        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        start_time = time.time()
        try:
            # RAGë¥¼ ì‚¬ìš©í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            if self.enable_rag and self.vector_search:
                print(f"    ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹œì‘...")
                vector_context = self.vector_search.get_context_for_llm(
                    query=input_text,
                    max_context_length=2000
                )
                print(f"    âœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(vector_context)} ë¬¸ì")
                reference_text = vector_context
            else:
                # ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                current_time = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
                time_info = f"í˜„ì¬ ì‹œê°„ì€ {current_time}ì…ë‹ˆë‹¤.\n\n"
                reference_text = time_info + (search_text if search_text else "")

            # ëŒ€í™” ê¸°ë¡ ì •ê·œí™”
            normalized_chat_list = []
            if chat_list and len(chat_list) > 0:
                for chat in chat_list:
                    normalized_chat = {
                        "index": chat.get("index"),
                        "input_data": chat.get("input_data"),
                        "output_data": self._normalize_escape_chars(chat.get("output_data", ""))
                    }
                    normalized_chat_list.append(normalized_chat)
            else:
                normalized_chat_list = chat_list

            # ìºë¦­í„° ì •ë³´ ì„¤ì •
            self.character_info = BaseConfig.OfficePrompt(
                name = self.data.get("character_name", "AI Assistant"),
                context = self.data.get("character_setting", "Helpful AI assistant"),
                reference_data = reference_text,
                user_input = input_text,
                chat_list = normalized_chat_list,
            )

            # RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
            if self.enable_rag and self.vector_search:
                prompt = build_llama3_prompt_with_rag(
                    character_info=self.character_info,
                    vector_context=reference_text
                )
            else:
                prompt = build_llama3_prompt(character_info=self.character_info)
            # ê· í˜• ì¡íŒ ì„¤ì •ìœ¼ë¡œ ìˆ˜ì •
            self.config = BaseConfig.LlamaGenerationConfig(
                prompt = prompt,
                max_tokens = 1024,                  # ì ì ˆí•œ í† í° ìˆ˜
                temperature = 0.7,                  # ì˜¨ë„ ì ì ˆíˆ ì¡°ì •
                top_p = 0.9,                        # top_p ë³µì›
                min_p = 0.1,                        # min_p ë³µì›
                typical_p = 1.0,                    # typical_p ì¶”ê°€
                tfs_z = 1.1,                        # tfs_z ë³µì›
                repeat_penalty = 1.08,              # repeat_penalty ë³µì›
                frequency_penalty = 0.1,            # frequency_penalty ë³µì›
                presence_penalty = 0.1,             # presence_penalty ë³µì›
                seed = None,                        # ì‹œë“œ ì—†ìŒ (ë‹¤ì–‘ì„± í™•ë³´)
            )
            chunks = []
            for text_chunk in self.create_streaming_completion(config = self.config):
                chunks.append(text_chunk)
            
            result = "".join(chunks)
            generation_time = time.time() - start_time
            return result

        except Exception as e:
            generation_time = time.time() - start_time
            print(f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} (ì†Œìš” ì‹œê°„: {generation_time:.2f}ì´ˆ)")
            return f"ì˜¤ë¥˜: {str(e)}"

    def _normalize_escape_chars(self, text: str) -> str:
        """
        ì´ìŠ¤ì¼€ì´í”„ ë¬¸ìê°€ ì¤‘ë³µëœ ë¬¸ìì—´ì„ ì •ê·œí™”í•©ë‹ˆë‹¤
        """
        if not text:
            return ""
            
        # ì´ìŠ¤ì¼€ì´í”„ëœ ê°œí–‰ë¬¸ì ë“±ì„ ì •ê·œí™”
        result = text.replace("\\n", "\n")
        result = result.replace("\\\\n", "\n")
        result = result.replace('\\"', '"')
        result = result.replace("\\\\", "\\")
        
        return result


if __name__ == "__main__":
    """
    RAG ê¸°ëŠ¥ì´ ì¶”ê°€ëœ Llama ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
    ì‹¤ì œ ì‚¬ìš©ìë“¤ì´ ê°•ì•„ì§€ ê±´ê°• ë¬¸ì œì— ëŒ€í•´ ì§ˆë¬¸í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ê³ 
    ë‹µë³€ë“¤ì„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    import logging
    from datetime import datetime
    
    # ë¡œê¹… ì„¤ì • - printë¬¸ ëŒ€ì‹  ì‚¬ìš©
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('test_results.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤ - ì‹¤ì œ ë°˜ë ¤ê²¬ ì£¼ì¸ë“¤ì´ í•  ë§Œí•œ ì§ˆë¬¸ë“¤
    test_questions = [
        {
            "category": "ì‘ê¸‰ìƒí™©",
            "question": "ìš°ë¦¬ ê°•ì•„ì§€ê°€ ê°‘ìê¸° í† í•˜ê³  ì„¤ì‚¬ë¥¼ í•´ìš”. ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?",
            "user_profile": "ì´ˆë³´ ê²¬ì£¼"
        },
        {
            "category": "í”¼ë¶€ì§ˆí™˜",
            "question": "ê°•ì•„ì§€ í„¸ì´ ë¹ ì§€ê³  ê°€ë ¤ì›Œí•´ìš”. í”¼ë¶€ê°€ ë¹¨ê°›ê²Œ ë˜ì—ˆëŠ”ë° í”¼ë¶€ì—¼ì¸ê°€ìš”?",
            "user_profile": "ê²½í—˜ ìˆëŠ” ê²¬ì£¼"
        },
        {
            "category": "ëˆˆì§ˆí™˜",
            "question": "ê°•ì•„ì§€ ëˆˆì—ì„œ ë…¸ë€ ëˆˆê³±ì´ ê³„ì† ë‚˜ì™€ìš”. ê²°ë§‰ì—¼ì¼ê¹Œìš”?",
            "user_profile": "ê±±ì • ë§ì€ ê²¬ì£¼"
        },
        {
            "category": "ì¹˜ê³¼ë¬¸ì œ",
            "question": "ê°•ì•„ì§€ ì…ì—ì„œ ëƒ„ìƒˆê°€ ì‹¬í•˜ê²Œ ë‚˜ê³  ì‡ëª¸ì´ ë¹¨ê°œìš”. ì¹˜ì„ ë•Œë¬¸ì¼ê¹Œìš”?",
            "user_profile": "ì„±ê²¬ ë³´í˜¸ì"
        },
        {
            "category": "ì™¸ìƒ",
            "question": "ì‚°ì±… ì¤‘ì— ê°•ì•„ì§€ê°€ ë‹¤ë¦¬ë¥¼ ì ˆëšê±°ë ¤ìš”. ë°œê°€ë½ ì‚¬ì´ë¥¼ í™•ì¸í•´ë´ì•¼ í• ê¹Œìš”?",
            "user_profile": "í™œë™ì ì¸ ê²¬ì£¼"
        },
        {
            "category": "ë‚´ê³¼ì§ˆí™˜",
            "question": "ê°•ì•„ì§€ê°€ ë¬¼ì„ ë§ì´ ë§ˆì‹œê³  ì†Œë³€ì„ ìì£¼ ë´ìš”. ë‹¹ë‡¨ë³‘ì´ ì˜ì‹¬ë˜ë‚˜ìš”?",
            "user_profile": "ë…¸ë ¹ê²¬ ë³´í˜¸ì"
        },
        {
            "category": "í–‰ë™ë¬¸ì œ",
            "question": "ê°•ì•„ì§€ê°€ ê³„ì† í•œ ê³³ì„ í•¥ì•„ì„œ ìƒì²˜ê°€ ìƒê²¼ì–´ìš”. ìŠ¤íŠ¸ë ˆìŠ¤ ë•Œë¬¸ì¼ê¹Œìš”?",
            "user_profile": "ë¶„ë¦¬ë¶ˆì•ˆ ê²½í—˜ì"
        },
        {
            "category": "ì˜ˆë°©ì ‘ì¢…",
            "question": "ê°•ì•„ì§€ ì¢…í•©ë°±ì‹  ì ‘ì¢… í›„ ê¸°ë ¥ì´ ì—†ì–´ìš”. ë¶€ì‘ìš©ì¸ê°€ìš”?",
            "user_profile": "ì‹ ì¤‘í•œ ê²¬ì£¼"
        }
    ]
    
    def run_comprehensive_test():
        """í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥"""
        logger.info("=== RAG ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        
        try:
            # ëª¨ë¸ ì´ˆê¸°í™” (printë¬¸ ëŒ€ì‹  logging ì‚¬ìš©)
            original_print = print
            def quiet_print(*args, **kwargs):
                pass
            
            # print í•¨ìˆ˜ë¥¼ ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”
            import builtins
            builtins.print = quiet_print
            
            # ëª¨ë¸ ì´ˆê¸°í™”
            model = LlamaModel(enable_rag=True)
            
            # print í•¨ìˆ˜ ë³µì›
            builtins.print = original_print
            
            logger.info("ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
            test_results = []
            
            for i, test_case in enumerate(test_questions, 1):
                logger.info(f"í…ŒìŠ¤íŠ¸ {i}/{len(test_questions)} ì‹¤í–‰ ì¤‘: {test_case['category']}")
                
                start_time = datetime.now()
                
                # ì‘ë‹µ ìƒì„± (printë¬¸ ë¹„í™œì„±í™”)
                builtins.print = quiet_print
                try:
                    response = model.generate_response(
                        input_text=test_case["question"],
                        search_text="",
                        chat_list=[]
                    )
                finally:
                    builtins.print = original_print
                
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                
                # ê²°ê³¼ ì €ì¥
                result = {
                    "test_number": i,
                    "category": test_case["category"],
                    "user_profile": test_case["user_profile"],
                    "question": test_case["question"],
                    "response": response,
                    "response_time": duration,
                    "timestamp": start_time.strftime("%Y-%m-%d %H:%M:%S")
                }
                test_results.append(result)
                
                logger.info(f"í…ŒìŠ¤íŠ¸ {i} ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ)")
            
            # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„±
            markdown_content = generate_markdown_report(test_results, model)
            
            # íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rag_test_results_{timestamp}.md"
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            logger.info("=== ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def generate_markdown_report(test_results, model):
        """ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±"""
        timestamp = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„")
        
        # RAG ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´
        rag_status = "í™œì„±í™”" if model.enable_rag and model.vector_search else "ë¹„í™œì„±í™”"
        vector_status = model.vector_search.get_connection_status() if model.vector_search else {}
        
        md_content = f"""# ğŸ• ë°˜ë ¤ê²¬ AI ìƒë‹´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        
## ğŸ“‹ í…ŒìŠ¤íŠ¸ ê°œìš”
- **í…ŒìŠ¤íŠ¸ ì¼ì‹œ**: {timestamp}
- **í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜**: {len(test_results)}ê°œ
- **RAG ì‹œìŠ¤í…œ**: {rag_status}
- **ëª¨ë¸**: {model.model_id}

## ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì •
- **ë²¡í„° DB ì—°ê²°**: {vector_status.get('status', 'Unknown')}
- **ë¬¸ì„œ ìˆ˜**: {vector_status.get('document_count', 0):,}ê°œ
- **ì‚¬ìš© ê°€ëŠ¥í•œ ì§„ë£Œê³¼**: {', '.join(vector_status.get('available_departments', []))}

---

## ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½

| í…ŒìŠ¤íŠ¸ ë²ˆí˜¸ | ì¹´í…Œê³ ë¦¬ | ì‚¬ìš©ì í”„ë¡œí•„ | ì‘ë‹µ ì‹œê°„ |
|------------|---------|-------------|----------|
"""
        
        # ìš”ì•½ í…Œì´ë¸” ì¶”ê°€
        for result in test_results:
            md_content += f"| {result['test_number']} | {result['category']} | {result['user_profile']} | {result['response_time']:.2f}ì´ˆ |\n"
        
        md_content += "\n---\n\n## ğŸ“ ìƒì„¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼\n\n"
        
        # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë³„ ìƒì„¸ ê²°ê³¼
        for result in test_results:
            md_content += f"""### ğŸ” í…ŒìŠ¤íŠ¸ {result['test_number']}: {result['category']}

**ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„**: {result['user_profile']}  
**â° í…ŒìŠ¤íŠ¸ ì‹œê°„**: {result['timestamp']}  
**âš¡ ì‘ë‹µ ì‹œê°„**: {result['response_time']:.2f}ì´ˆ

#### ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸
> {result['question']}

#### ğŸ¤– AI ì‘ë‹µ
{result['response']}

---

"""
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        avg_response_time = sum(r['response_time'] for r in test_results) / len(test_results)
        categories = list(set(r['category'] for r in test_results))
        
        md_content += f"""## ğŸ“ˆ í†µê³„ ì •ë³´

### â±ï¸ ì„±ëŠ¥ ì§€í‘œ
- **í‰ê·  ì‘ë‹µ ì‹œê°„**: {avg_response_time:.2f}ì´ˆ
- **ìµœë¹ ë¥¸ ì‘ë‹µ**: {min(r['response_time'] for r in test_results):.2f}ì´ˆ
- **ìµœëŠë¦° ì‘ë‹µ**: {max(r['response_time'] for r in test_results):.2f}ì´ˆ

### ğŸ“‚ í…ŒìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬
{', '.join(categories)}

### ğŸ¥ ì§„ë£Œê³¼ë³„ ë¶„ì„
"""
        
        # ì§„ë£Œê³¼ë³„ í†µê³„ (ê°„ë‹¨íˆ)
        category_stats = {}
        for result in test_results:
            cat = result['category']
            if cat not in category_stats:
                category_stats[cat] = {'count': 0, 'total_time': 0}
            category_stats[cat]['count'] += 1
            category_stats[cat]['total_time'] += result['response_time']
        
        for category, stats in category_stats.items():
            avg_time = stats['total_time'] / stats['count']
            md_content += f"- **{category}**: {stats['count']}ê°œ í…ŒìŠ¤íŠ¸, í‰ê·  {avg_time:.2f}ì´ˆ\n"
        
        md_content += f"""

---

## ğŸ” RAG ì‹œìŠ¤í…œ ë¶„ì„

### ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥
- **ê²€ìƒ‰ ì‹œìŠ¤í…œ**: ChromaDB
- **ì„ë² ë”© ëª¨ë¸**: ê¸°ë³¸ ì„ë² ë”©
- **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: ìµœëŒ€ 2000ì

### ì‹œìŠ¤í…œ ê¶Œì¥ì‚¬í•­
1. **ì‘ë‹µ í’ˆì§ˆ**: ë²¡í„° ê²€ìƒ‰ì„ í†µí•´ ê´€ë ¨ ì˜ë£Œ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©
2. **ì‘ë‹µ ì†ë„**: í‰ê·  {avg_response_time:.2f}ì´ˆë¡œ ì‹¤ìš©ì  ìˆ˜ì¤€
3. **ì •í™•ì„±**: ì „ë¬¸ì˜ ìƒë‹´ ê¶Œìœ  ë“± ì˜ë£Œ ì•ˆì „ì„± ê³ ë ¤

---

*ğŸ•’ ë³´ê³ ì„œ ìƒì„± ì‹œê°„: {timestamp}*  
*ğŸ¤– Generated by RAG-Enhanced Llama AI System*
"""
        
        return md_content
    
    # ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    run_comprehensive_test()